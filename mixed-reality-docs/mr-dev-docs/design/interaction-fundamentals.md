---
title: Инстинктивное взаимодействие
description: Платформа Mixed Reality построена на принципах простого инстинктивного взаимодействия.
author: shengkait
ms.author: shentan
ms.date: 04/11/2019
ms.topic: article
ms.localizationpriority: high
keywords: смешанная реальность, взгляд, нацеливание взглядом, взаимодействие, проектирование, hololens, ММR, мультимодальный, гарнитура смешанной реальности, гарнитура Windows Mixed Reality, гарнитура виртуальной реальности, HoloLens
ms.openlocfilehash: 55e23ac2fb802af599fb9cc7d771d89d6ba36c47
ms.sourcegitcommit: 8f141a843bcfc57e1b18cc606292186b8ac72641
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/19/2021
ms.locfileid: "110196419"
---
# <a name="introducing-instinctual-interactions"></a>Знакомство с инстинктивным взаимодействием

![Манипулирование на большом расстоянии с помощью рук](images/04_InteractionFundamentals.png)

Платформа Mixed Reality (MR) построена на принципах простого инстинктивного взаимодействия. Мы сделали три шага, чтобы разработчики и конструкторы приложений могли предоставлять своим клиентам простые и понятные механизмы взаимодействия. 

Сначала мы убедились, что наши датчики и технологии ввода объединены в многомодальные модели взаимодействия. Такие модели взаимодействия включают отслеживание рук и взгляда, а также ввод на естественном языке. Мы ведем исследования, создаем проекты и разрабатываем решения на базе мультимодальной платформы, а не на основе индивидуальных входных данных. Такой подход — ключ к созданию возможностей инстинктивного взаимодействия.

Во-вторых, мы учитываем, что многие разработчики ориентированы на несколько устройств HoloLens, таких как HoloLens 2 и HoloLens (1-го поколения) или HoloLens и виртуальная реальность. Поэтому мы разработали модели взаимодействия для работы на разных устройствах, даже если технология ввода зависит от устройства. Например, при удаленном взаимодействии иммерсивной гарнитуры Windows с контроллером 6DoF и при удаленном взаимодействии с HoloLens 2 используются одинаковые возможности и шаблоны. Это упрощает разработку приложений на нескольких устройствах и позволяет сделать взаимодействия естественными. 

Хотя мы признаем, что в смешанной реальности возможны тысячи эффективных, увлекательных и магических взаимодействий, мы обнаружили, что преднамеренное использование единой модели взаимодействия в приложении — это лучший способ обеспечить успешную и комфортную работу пользователей. Поэтому в это руководство по взаимодействию мы включили три вещи.
* Специальные указания по трем основным моделям взаимодействия, а также по компонентам и шаблонам, необходимым для каждого.
* Дополнительное описание других преимуществ, которые предоставляет наша платформа.
* Общие инструкции по выбору подходящей модели взаимодействия для конкретного сценария разработки.

## <a name="basic-hand-tracking-and-instinctual-interactions-demo"></a>Основы отслеживания рук и инстинктивное взаимодействие — демонстрация

Ознакомьтесь с видеороликом **Designing Holograms - Head Tracking and Eye Tracking** (Создание голограмм — отслеживание головы и взгляда) ниже, а затем перейдите к более конкретным темам:

> [!VIDEO https://channel9.msdn.com/Shows/Docs-Mixed-Reality/Microsofts-Designing-Holograms-Hand-Tracking-Chapter/player]

*Это видео из приложения Designing Holograms для HoloLens 2. Скачайте и воспользуйтесь всеми его возможностями [здесь](https://aka.ms/dhapp).*

## <a name="multimodal-interaction-models"></a>Модели мультимодальных взаимодействий

Основываясь на наших исследованиях и отзывах клиентов, мы определили, что для реализации большинства возможностей смешанной реальности подходят три основные модели взаимодействия. Во многих отношениях модель взаимодействия — это модель мышления пользователя для выполнения своего рабочего процесса. Каждая из этих моделей взаимодействия оптимизирована для ряда потребностей клиентов. При правильном применении каждая из них является простой, мощной и удобной в использовании. 

Диаграмма ниже представляет собой упрощенный обзор. Подробная информация об использовании каждой модели взаимодействия приведена ниже на страницах с изображениями и примерами кода. 

<br>
<table>
    <colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    </colgroup>
    <tr>
        <td><strong>Модель</strong></td>
        <td><strong>Примеры сценариев</strong></td>
        <td><strong>Совпадение</strong></td>
        <td><strong>Оборудование</strong></td>
    </tr>
    <tr>
        <td><a href="hands-and-tools.md">Руки и контроллеры движения</a></td>
        <td>Возможности трехмерного пространства, такие как пространственная структура и конструктор, манипулирование содержимым или симуляция.</td>
        <td>В сочетании с системой голосовой связи, отслеживанием взгляда или направления головы отлично подходит для новых пользователей. Низкий порог вхождения. Последовательное взаимодействие с пользователем для отслеживания рук и контроллеров 6DoF.</td>
        <td>HoloLens 2<br>Иммерсивные гарнитуры</td>
    </tr>
    <tr>
        <td><a href="hands-free.md">Режимы без использования рук</a></td>
        <td>Контекстуальные возможности, когда руки пользователя заняты, например, обучением на рабочем месте и обслуживанием.</td>
        <td>Требуется некоторое обучение. Если руки должны быть свободны, устройство обеспечивает удобное управление с помощью голоса и естественного языка.</td>
        <td>HoloLens 2<br>HoloLens (1-го поколения)<br>Иммерсивные гарнитуры</td>
    </tr>
    <tr>
        <td><a href="gaze-and-commit.md">Взгляд и фиксация</a></td>
        <td>Взаимодействия по щелчку, например трехмерные презентации, демонстрации.</td>
        <td>Требуется обучение на HMD, но не на мобильном. Лучшее для доступных контроллеров. Лучшее для HoloLens (1-го поколения)</td>
        <td>HoloLens 2<br>HoloLens (1-го поколения)<br>Иммерсивные гарнитуры<br>Mobile AR</td>
    </tr>
</table>
<br>

Чтобы избежать несоответствий в механизме взаимодействия с пользователем, на всех этапах следуйте инструкциям, касающимся одной и той же модели.

В следующих разделах рассматриваются этапы выбора и реализации одной из этих моделей взаимодействия.  
 
### <a name="by-the-end-of-this-page-youll-understand-our-guidance-on"></a>Ознакомившись с инструкциями на этой странице, вы узнаете, как выполнять следующие задачи:
 
* Выбор модели взаимодействия для вашего клиента
* Реализация модели взаимодействия
* Переход между моделями взаимодействия
* Следующие шаги конструктора


## <a name="choose-an-interaction-model-for-your-customer"></a>Выберите модель взаимодействия для вашего клиента

Как правило, разработчики и проектировщики решений продумывают типы взаимодействия, ориентируясь на потребности клиента. Чтобы поощрить такой подход к проектированию, мы рекомендуем следовать приведенным ниже инструкциям по выбору модели взаимодействия, оптимизированной для вашего клиента.

### <a name="why-follow-this-guidance"></a>Почему нужно следовать руководству?

* Мы проверяем наши модели взаимодействия по объективным и субъективным критериям, таким как физические и когнитивные трудозатраты, простота восприятия и обучаемость. 
* Так как есть множество способов взаимодействия, визуальные и звуковые возможности, а также поведение объекта отличаются в зависимости от конкретной модели.  
* Объединение частей нескольких моделей взаимодействия создает риск конкурирующих возможностей, таких как одновременное срабатывание телекинеза и курсора направления головы. Это может быть сложным для восприятия пользователей.

Ниже приведены несколько примеров того, как возможности и расширения функциональности оптимизируются для каждой модели взаимодействия. Мы часто видим, что у новых пользователей возникают похожие вопросы, например: "_Как я узнаю, что система работает_, _как мне узнать, что я могу сделать_, и _как узнать, поняла ли она мое действие?_ "

<br>

<table>
    <colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    </colgroup>
    <tr>
        <td><strong>Модель</strong></td>
        <td><strong>Как мне узнать, что она работает?</strong></td>
        <td><strong>Как мне узнать, что я могу сделать?</strong></td>
        <td><strong>Как мне узнать, что я только что сделал?</strong></td>
    </tr>
    <tr>
        <td><a href="hands-and-tools.md">Руки и контроллеры движения</a></td>
        <td>Я вижу виртуальную руку, возможность курсора для кончика пальца или телекинез либо лучи контроллера движений.</td>
        <td>Когда моя рука рядом с объектом, я вижу захватные маркеры или появление ограничивающего прямоугольника.</td>
        <td>Я слышу звуковые сигналы и вижу анимацию при захвате и выпуске.</td>
    </tr>
    <tr>
        <td><a href="gaze-and-commit.md">Направление головы и фиксация</a></td>
        <td>Я вижу курсор в центре моей видимой зоны.</td>
        <td>Курсор изменяется над определенными объектами.</td>
        <td>Я вижу визуальные подтверждения или слышу звуковые подтверждения при определенных действиях.</td>
    </tr>   
    <tr>
        <td><a href="hands-free.md">Без использования рук (Направление головы и остановка)</a></td>
        <td>Я вижу курсор в центре моей видимой зоны.</td>
        <td>Я вижу индикатор хода выполнения, когда останавливаюсь на интерактивном объекте.</td>
        <td>Я вижу визуальные подтверждения или слышу звуковые подтверждения при определенных действиях.</td>
    </tr>
    <tr>
        <td><a href="hands-free.md">Без использования рук (Голосовые команды)</a></td>
        <td>Я вижу индикатор прослушивания и заголовки, которые показывают, что слышала система.</td>
        <td>Я получаю голосовые приглашения и указания. Когда я говорю: "Что можно говорить?" Я вижу отзыв.</td>
        <td>Я вижу или слышу визуальные и звуковые подтверждения, когда даю команду, или пользовательский интерфейс устраняет неоднозначность, когда это необходимо.</a></td>
    </tr>
</table>

### <a name="below-are-questions-that-weve-found-help-teams-select-an-interaction-model"></a>Ниже приведены вопросы, которые, по нашему мнению, помогут командам выбрать модель взаимодействия.
 
1.  Вопрос.  Хотят ли мои пользователи хотят дотрагиваться до голограмм и выполнять точные голографические операции?<br><br>
Ответ.  Если это так, ознакомьтесь с моделью взаимодействия "Руки и контроллеры движений" для точного нацеливания и манипулирования.
 
2.  Вопрос.  Должны ли руки пользователей быть свободны, чтобы они могли выполнять определенные задачи в реальности?<br><br>
Ответ.  Если это так, обратите внимание на модель "Без использования рук", которая обеспечивает отличные возможности взаимодействия на основе направления взгляда и голосовых команд.
 
3.  Вопрос.  У моих пользователей есть время, чтобы освоить работу с моим приложением MR, или им нужно взаимодействие с минимальным периодом обучения?<br><br>
Ответ.  Рекомендуем использовать модель "Контроллеры движения и жестов". Обучение работе с ней не занимает много времени, и она обеспечивает наиболее интуитивно простое взаимодействие, если пользователи могут задействовать руки.
 
4.  Вопрос.  Используют ли мои пользователи контроллеры движения для указывания и операций?<br><br>
Ответ.  Для модели "Руки и контроллеры движений" предоставляются полные инструкции, которые обеспечат успешную работу с контроллерами движения.
 
5.  Вопрос.  Используют ли мои пользователи контроллер специальных возможностей или обычный контроллер Bluetooth, такой как кликер?<br><br>
Ответ.  Мы рекомендуем использовать модель "Направление головы и подтверждение" для всех контроллеров, которые не отслеживаются. Она обеспечивает полное взаимодействие пользователя с системой с помощью простого механизма нацеливания и подтверждения. 
 
6.  Вопрос. Мои пользователи будут работать только с помощью "переходов" (например, в среде, похожей на слайд-шоу в трехмерном пространстве), не используя навигацию по многоуровневым элементам управления пользовательского интерфейса?<br><br>
Ответ.  Если пользователям не нужно управлять большим числом объектов пользовательского интерфейса, "Направление головы и фиксация" предлагает удобный для изучения вариант, в котором пользователям не нужно беспокоиться о нацеливании. 
 
7.  Вопрос.  Используют ли мои пользователи как HoloLens (1-го поколения), так и HoloLens 2 или иммерсивные гарнитуры (гарнитуры виртуальной реальности) Windows Mixed Reality?<br><br>
Ответ.  Так как "Направление головы и фиксация" — это модель взаимодействия для HoloLens (1-го поколения), мы рекомендуем, чтобы разработчики, которые используют HoloLens (1-го поколения), применяли модель "Направление головы и фиксация" для любых функций или режимов, с которыми пользователи будут работать на гарнитуре HoloLens (1-го поколения). Подробные сведения о том, что нужно для комфортной работы с HoloLens разных поколений, см. в следующем разделе *Переходные модели взаимодействия*.
 
8.  Вопрос. Есть ли различия между моделями для пользователей, которые обычно работают в большом пространстве или перемещаются между разными пространствами, и пользователей, которые обычно работают в одном пространстве?<br><br>
Ответ.  Любая из моделей взаимодействия подойдет этим пользователям.  

> [!NOTE]
> [В ближайшее время](../out-of-scope/news.md) появятся дополнительные руководства, касающиеся дизайна приложения.


## <a name="transitioning-interaction-models"></a>Переходные модели взаимодействия
Иногда нужно использовать более одной модели взаимодействия. Например, при создании приложения используется модель _Контроллеры движений и жестов_, но вам нужно применить режим "без использования рук" для выездных техников. Если ваш интерфейс требует нескольких моделей взаимодействия, пользователи могут столкнуться с трудностями при переходе от одной модели к другой, особенно если они недавно начали использовать смешанную реальность.

> [!Note]
> Мы постоянно предоставляем разработчикам и дизайнерам дополнительные рекомендации относительно способов, сценариев и целей использования нескольких моделей взаимодействия MR.
 

## <a name="see-also"></a>См. также статью
* [Комфорт](comfort.md)
* [Взаимодействие на основе глаз](eye-gaze-interaction.md)
* [Отслеживание глаз в HoloLens 2](eye-tracking.md)
* [Взгляд и фиксация](gaze-and-commit.md)
* [Взгляд и остановка](gaze-and-dwell.md)
* [Руки: непосредственное манипулирование](direct-manipulation.md)
* [Руки: жесты](gaze-and-commit.md#composite-gestures)
* [Руки: наведение и фиксация](point-and-commit.md)
* [Инстинктивное взаимодействие](interaction-fundamentals.md)
* [Контроллеры движения](motion-controllers.md)
* [Пространственное сопоставление](spatial-mapping.md)
* [Проектирование пространственного звука](spatial-sound-design.md)
* [Голосовой ввод](voice-input.md)


