---
ms.openlocfilehash: e84290bb2fcbdefa3a6c29d4f79c2cbb307ce439
ms.sourcegitcommit: bea83261bf9ce7a27a618e5bc54dc4d7711f5435
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/19/2021
ms.locfileid: "130153285"
---
# <a name="project-settings"></a>[Параметры проекта](#tab/project)

### <a name="1-review-the-common-porting-steps-listed-above"></a>1. Ознакомьтесь с общими действиями по переносу, перечисленными выше.

Чтобы убедиться, что среда разработки настроена правильно, ознакомьтесь с приведенными выше общими действиями. на шаге #3, если вы используете Visual Studio, следует выбрать рабочую нагрузку " **разработка игр с помощью Unity** ". Вы можете отменить выбор компонента "необязательный редактор Unity", так как вы будете устанавливать более новую версию Unity на следующем шаге.

### <a name="2-upgrade-to-the-latest-public-build-of-unity-with-windows-mr-support"></a>2. обновление до последней общедоступной сборки Unity с поддержкой Windows MR
1. Скачайте последнюю [рекомендуемую общедоступную сборку Unity](../../install-the-tools.md) с поддержкой смешанной реальности.
2. Сохраните копию проекта, прежде чем приступить к работе
3. Ознакомьтесь с [документацией](https://docs.unity3d.com/Manual/UpgradeGuides.html) , доступной в Unity, при обновлении, если проект создан на основе более старой версии Unity.
4. Следуйте [инструкциям](https://docs.unity3d.com/Manual/APIUpdater.html) на сайте Unity для использования автоматических средств обновления API.
5. Проверьте, есть ли дополнительные изменения, необходимые для запуска проекта, и выполните все оставшиеся ошибки и предупреждения. 

> [!Note] 
> Если по промежуточного слоя от вас зависит, убедитесь, что вы используете последний выпуск (Дополнительные сведения см. на шаге 3 ниже).

### <a name="3-upgrade-your-middleware-to-the-latest-versions"></a>3. Обновите по промежуточного слоя до последних версий

При любом обновлении Unity есть хороший шанс, что вам потребуется обновить один или несколько пакетов по промежуточного слоя, от которых зависит ваша игра или приложение. Кроме того, обновление по промежуточного слоя повышает вероятность успеха на протяжении всего процесса переноса.

### <a name="4-target-your-application-to-run-on-win32"></a>4. Назначение приложения для запуска в Win32

Изнутри приложения Unity:

* перейдите к Параметры сборки файла >
* Выберите "компьютер, Mac, автономная версия Linux"
* Присвоить целевой платформе значение "Windows"
* Установка архитектуры "x86" выберите параметр "платформа"

> [!NOTE] 
> Если приложение имеет какие-либо зависимости от служб для конкретных устройств, таких как сопоставление из Steam, необходимо отключить их на этом шаге. вы можете подключить к аналогичным службам, которые Windows предоставляет позже.

### <a name="5-setup-your-windows-mixed-reality-hardware"></a>5. настройка оборудования Windows Mixed Reality
1. Ознакомьтесь с этапами [настройки иммерсивного головного телефона](/windows/mixed-reality/enthusiast-guide/before-you-start
)
2. сведения об [использовании симулятора Windows Mixed Reality](../../advanced-concepts/using-the-windows-mixed-reality-simulator.md) и [навигации по Windows Mixed Reality домой](../../../discover/navigating-the-windows-mixed-reality-home.md)

### <a name="6-target-your-application-to-run-on-windows-mixed-reality"></a>6. Настройка приложения для запуска на Windows Mixed Reality
1. Во-первых, необходимо удалить или условно откомпилировать любую другую поддержку библиотеки, относящуюся к конкретному пакету SDK для VR. Эти активы часто изменяют параметры и свойства проекта способами, несовместимыми с другими пакетами SDK для VR, например Windows Mixed Reality.
    * например, если проект ссылается на пакет SDK для стеамвр, необходимо обновить проект, чтобы вместо этого использовать общие api-интерфейсы среды Unity, поддерживающие как Windows Mixed Reality, так и стеамвр.
    * В ближайшее время ожидается процедура, исключающая другие пакеты SDK для VR.
2. в проекте Unity [нацеливание на пакет SDK для Windows 10](../../unity/tutorials/holograms-100.md#target-windows-10-sdk)
3. Для каждой сцены [Настройте камеру](../../unity/tutorials/holograms-100.md#chapter-2---setup-the-camera) .

### <a name="7-use-the-stage-to-place-content-on-the-floor"></a>7. использование этапа для размещения содержимого в этаже

Вы можете создавать возможности смешанной реальности во множестве различных [возможностей масштабирования](../../../design/coordinate-systems.md).

Если вы переносите **интерфейсное масштабирование**, необходимо убедиться, что Unity настроен на **стационарный** тип пространства отслеживания:

```cs
XRDevice.SetTrackingSpaceType(TrackingSpaceType.Stationary);
```

Приведенный выше код устанавливает мировую систему координат Unity для трассировки [стационарной рамки ссылки](../../../design/coordinate-systems.md#spatial-coordinate-systems). В режиме стационарного отслеживания содержимое, помещенное в редактор непосредственно перед расположением по умолчанию камеры (переадресация — Z), отображается перед пользователем при запуске приложения. Чтобы перецентрировать исходный элемент пользователя, можно вызвать [XR Unity. Метод Инпуттраккинг. recenter](https://docs.unity3d.com/ScriptReference/XR.InputTracking.Recenter.html) .

Если вы проведете переносные **средства или** **возможности масштабирования комнаты**, вы помещаете содержимое относительно пола. Вы указываете на этаж пользователя с помощью **[пространственного этапа](../../../design/coordinate-systems.md#spatial-coordinate-systems)**, который представляет определенное пользователем происхождение на уровне пола и дополнительную границу комнаты, настраивается во время первого запуска. Для этих возможностей необходимо убедиться, что для Unity задан тип пространства отслеживания **румскале** . Хотя Румскале является значением по умолчанию, необходимо явно задать его и убедиться, что вы получаете значение true, чтобы выявить ситуации, в которых пользователь переместил компьютер за пределы разкалиброванной комнаты.

```cs
if (XRDevice.SetTrackingSpaceType(TrackingSpaceType.RoomScale))
{
    // RoomScale mode was set successfully.  App can now assume that y=0 in Unity world coordinate represents the floor.
}
else
{
    // RoomScale mode was not set successfully.  App cannot make assumptions about where the floor plane is.
}
```

После того как приложение успешно установит тип пространства отслеживания Румскале, на этаж будут отображаться содержимое, помещенное на плоскости y = 0. Источник в точке (0, 0, 0) будет определять конкретное место в этаже, где пользователь стояли во время настройки комнаты, с параметром-Z, представляющим Направление переадресации во время установки.

В коде скрипта можно вызвать метод Трижетжеометри, так как тип UnityEngine. экспериментальный. XR. граничное используется для получения граничного многоугольника, указывая тип границы Траккедареа. Если пользователь определил границу (вы получаете список вершин), вы можете легко предоставить пользователю **возможности масштабирования комнаты** , где они могут пройти по создаваемой сцене.

Система автоматически визуализирует границу, когда пользователь его приближает. Приложению не нужно использовать этот многоугольник для отрисовки самой границы.

Дополнительные сведения см. на странице [системы координат на Unity](../../unity/coordinate-systems-in-unity.md) .

<!-- Some applications use a rectangle to constrain their interaction. Retrieving the largest inscribed rectangle is not directly supported in the UWP API or Unity. The example code linked to below shows how to find a rectangle within the traced bounds. It's heuristic-based so may not find the optimal solution, however, results are consistent with expectations. Parameters in the algorithm can be tuned to find more precise results at the cost of processing time. The algorithm is in a fork of the Mixed Reality Toolkit that uses the 5.6 preview MRTP version of Unity. This isn't publicly available. The code should be directly usable in 2017.2 and higher versions of Unity. The code will be ported to the current MRTK in the near future. -->

Пример результатов:

![Пример результатов](../../porting-apps/images/largestrectangle-400px.jpg)

Алгоритм основан на блоге, Даниэль Смилков: [самый крупный прямоугольник в многоугольнике](https://d3plus.org/blog/behind-the-scenes/2014/07/08/largest-rect/)

### <a name="8-work-through-your-input-model"></a>8. Работа с моделью ввода

Каждая игра или приложение, предназначенное для существующего ХМД, будет иметь набор входных данных, которые он обрабатывает, типы входных данных, необходимые для работы, и конкретные интерфейсы API, которые он вызывает для получения этих входных данных. Мы собрались сделать это как можно проще и простым, чтобы воспользоваться преимуществами входных данных, доступных в Windows Mixed Reality.

прочтите [руководство по переносу данных для Unity](../porting-guides.md?tabs=input) на соседней вкладке, где приводятся сведения о том, как Windows Mixed Reality предоставляет входные данные и как они сопоставлены с тем, что приложение может сделать сегодня.

### <a name="9-performance-testing-and-tuning"></a>9. Тестирование и настройка производительности

Windows Mixed Reality будут доступны на широком классе устройств, от высокопроизводительных компьютерных пк до широкого спектра основных пк на рынке. В зависимости от того, на каком рынке ориентирован целевой объект, существует существенная разница в доступных вычислениях и графических бюджетах для вашего приложения. Во время этого упражнения по переносу вы, скорее всего, используете ПК уровня "Премиум" и имели значительные бюджетные и графические графики, доступные для вашего приложения. Если вы хотите сделать приложение доступным более широкой аудитории, следует протестировать и профилировать приложение на [целевом оборудовании, которое вы хотите](/windows/mixed-reality/enthusiast-guide/windows-mixed-reality-minimum-pc-hardware-compatibility-guidelines)использовать.

[Unity](https://docs.unity3d.com/Manual/Profiler.html) и [Visual Studio](/visualstudio/profiling/index) включают в себя профилировщики производительности, а как [корпорация майкрософт](../../advanced-concepts/understanding-performance-for-mixed-reality.md) , так и [корпорация Intel](https://software.intel.com/articles/vr-content-developer-guide) публикуют рекомендации по профилированию и оптимизации производительности. Существует подробное обсуждение производительности, доступное [для понимания производительности смешанной реальности](../../advanced-concepts/understanding-performance-for-mixed-reality.md). кроме того, в разделе [Performance Рекомендации для unity](../../unity/performance-recommendations-for-unity.md)есть определенные сведения о unity.

# <a name="input-mapping"></a>[сопоставление входных данных;](#tab/input)

вы можете перенести логику ввода в Windows Mixed Reality с помощью одного из двух подходов: общих входных api-интерфейсов Unity и XR, охватывающих несколько платформ, или конкретного Windows. Головк. интерфейсы api ввода, которые предлагают более широкие данные для контроллеров движения и HoloLens руки.

> [!IMPORTANT]
> Если вы используете контроллеры HP reverbы G2, обратитесь к [этой статье](../../unity/unity-reverb-g2-controllers.md) для получения дополнительных инструкций по сопоставлению входных данных.

## <a name="unity-xr-input-apis"></a>Входные API-интерфейсы Unity XR

Для новых проектов рекомендуется использовать новые интерфейсы API ввода XR с самого начала. 

Дополнительные сведения об [API XR](https://docs.unity3d.com/Manual/xr_input.html)можно найти здесь.

## <a name="inputgetbuttongetaxis-apis"></a>Входные API-интерфейсы input. OnButton и AXIS

В настоящее время Unity использует общие интерфейсы API input. Окулус и input. Axis для предоставления входных данных для [пакета SDK](https://docs.unity3d.com/Manual/OculusControllers.html) и [пакета SDK для опенвр](https://docs.unity3d.com/Manual/OpenVRControllers.html). если ваши приложения уже используют эти api для ввода данных, это самый простой путь для поддержки контроллеров движения в Windows Mixed Reality: необходимо просто сопоставить кнопки и оси в диспетчере ввода.

Дополнительные сведения см. в разделе [Таблица соответствия кнопок и осей Unity](../../unity/motion-controllers-in-unity.md#unity-buttonaxis-mapping-table) , а также [Общие сведения об общих API Unity](../../unity/motion-controllers-in-unity.md#common-unity-apis-inputgetbuttongetaxis).

## <a name="windows-specific-xrwsainput-apis"></a>XR, характерный для Windows. Головк. Входные интерфейсы API

> [!CAUTION]
> Если в проекте используются какие-либо из XR. Поэтапные API-интерфейсы см. в них используется пакет SDK XR в будущих выпусках Unity. Для новых проектов мы рекомендуем использовать пакет SDK для XR с самого начала. Дополнительные сведения о [системе ввода и интерфейсах API XR](https://docs.unity3d.com/Manual/xr_input.html)можно найти здесь.

если приложение уже создает пользовательскую логику ввода для каждой платформы, вы можете использовать пространственные входные api Windows в пространстве имен **UnityEngine. XR. WSA. input** . Это позволяет получить доступ к дополнительным сведениям, таким как точность расположения или тип источника, что позволяет сообщать руки и контроллеры на HoloLens.

> [!NOTE]
> Если вы используете контроллеры HP reverbы G2, все входные API-интерфейсы будут продолжать работать, за исключением **интерактионсаурце. суппортстаучпад**, который вернет значение false без данных сенсорной панели.

Дополнительные сведения см. в [обзоре интерфейсов API UnityEngine. XR. WSA. Input](../../unity/motion-controllers-in-unity.md#windows-specific-apis-xrwsainput).

## <a name="grip-pose-vs-pointing-pose"></a>Захват захвата и указание объекта a

Windows Mixed Reality поддерживает контроллеры движения в различных конструктивных факторах, при этом структура каждого контроллера различается в связи между положением пользователя и естественным направлением "вперед", которое приложения должны использовать для указания при подготовке к просмотру контроллера.

Для лучшего представления этих контроллеров существует два вида элементов, которые можно исследовать для каждого источника взаимодействия:

* **захват**, представляющий местоположение ручного устройства, обнаруженного HoloLens, или карманный пк, удерживающий контроллер движения.
    * В современных головных гарнитурах эта цель лучше подходит для визуализации **руки пользователя** или **объекта, хранящегося в руки пользователя**, например технологий или «обойма».
    * **Позиция захвата**: Карманный центроид при удержании контроллера естественным образом настраивается влево или вправо для центрирования места внутри захвата.
    * **Правая ось ориентации захвата**: когда вы полностью открываете руку для формирования плоской задачи с 5-пальцем, луч, обычный для вашего кармана (вперед от левого кармана, назад от правого Palm)
    * **Прямая ось ориентации захвата**: когда вы частично закрываете руку (как при удержании контроллера), луч, который указывает на "Forward" через лампу, сформированную небегункными пальцами.
    * **Ось Up (вверх) для ориентации захвата**: ось Up, подразумеваемая правым и прямым определением.
    * Доступ к захвату можно получить через API-интерфейс ввода кросс-поставщика Unity (**[XR). Инпуттраккинг](https://docs.unity3d.com/ScriptReference/XR.InputTracking.html). жетлокалпоситион/вращение**) или через API Windows (**саурцестате. саурцепосе. трижетпоситион/вращение**, запрашивающий захват).
* **Указатель**, представляющий кончик контроллера, указывающий на пересылку.
    * Эта цель лучше использовать для райкаст при **наведении указателя на пользовательский интерфейс** при отрисовке самой модели контроллера.
    * в настоящее время указатель a доступен только через интерфейс API Windows (**саурцестате. саурцепосе. трижетпоситион/вращение**, запрашивающий указатель a).

Эти координаты представляются в универсальных координатах Unity.