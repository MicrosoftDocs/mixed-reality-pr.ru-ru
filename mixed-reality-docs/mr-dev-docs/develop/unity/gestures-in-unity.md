---
title: Жесты в Unity
description: Узнайте, как принять меры для вашего взгляда в Unity с помощью ввода жестов, используя XR и общие API кнопок и осей.
author: hferrone
ms.author: alexturn
ms.date: 12/1/2020
ms.topic: article
keywords: жесты, Unity, Взгляните, ввод, гарнитура смешанной реальности, гарнитура Windows Mixed Reality, гарнитура виртуальной реальности, МРТК, набор средств смешанной реальности
ms.openlocfilehash: 2a968235abaeff9187580b7f5f77263b27c38c28
ms.sourcegitcommit: a1bb77f729ee2e0b3dbd1c2c837bb7614ba7b9bd
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 01/14/2021
ms.locfileid: "98192973"
---
# <a name="gestures-in-unity"></a>Жесты в Unity

Существует два основных способа выполнения действий с вашим [взглядом в Unity](gaze-in-unity.md), [жестами](../../design/gaze-and-commit.md#composite-gestures) и [контроллерами движения](../../design/motion-controllers.md) в HoloLens и иммерсивное ХМД. Доступ к данным для обоих источников пространственных данных осуществляется через одни и те же API в Unity.

Unity предоставляет два основных способа доступа к данным пространственного ввода для Windows Mixed Reality. Общие *входные API-интерфейсы input. "input. Axis"* работают в нескольких пакетах SDK для Unity XR, а API *интерактионманажер/GestureRecognizer* , относящийся к Windows Mixed Reality, предоставляет полный набор пространственных входных данных.

## <a name="high-level-composite-gesture-apis-gesturerecognizer"></a>Интерфейсы API составных жестов высокого уровня (GestureRecognizer)

**Пространство имен:** *UnityEngine. XR. WSA. Input*<br>
**Типы**: *GestureRecognizer*, *жестуресеттингс*, *интерактионсаурцекинд*

Приложение может также распознать составные жесты более высокого уровня для пространственных источников входных данных, а также жесты касания, удержания, манипуляции и перемещения. Эти составные жесты можно распознавать как в движении, [так и в](../../design/gaze-and-commit.md#composite-gestures) [контроллерах движения](../../design/motion-controllers.md) с помощью GestureRecognizer.

Каждое событие жеста в GestureRecognizer предоставляет Саурцекинд для входных данных, а также целевой заголовочный элемент на момент события. Некоторые события предоставляют дополнительные сведения, относящиеся к контексту.

Для записи жестов с помощью распознавателя жестов требуется всего несколько действий:
1. Создание нового распознавателя жестов
2. Укажите, какие жесты следует отслеживать
3. Подписываться на события для этих жестов
4. Начать запись жестов

### <a name="create-a-new-gesture-recognizer"></a>Создание нового распознавателя жестов

Чтобы использовать *GestureRecognizer*, необходимо создать *GestureRecognizer*:

```cs
GestureRecognizer recognizer = new GestureRecognizer();
```

### <a name="specify-which-gestures-to-watch-for"></a>Укажите, какие жесты следует отслеживать

Укажите жесты, которые вас интересуют, с помощью *сетрекогнизаблежестурес ()*:

```cs
recognizer.SetRecognizableGestures(GestureSettings.Tap | GestureSettings.Hold);
```

### <a name="subscribe-to-events-for-those-gestures"></a>Подписываться на события для этих жестов

Подпишитесь на события для тех жестов, которые вас интересуют.

```cs
void Start()
{
    recognizer.Tapped += GestureRecognizer_Tapped;
    recognizer.HoldStarted += GestureRecognizer_HoldStarted;
    recognizer.HoldCompleted += GestureRecognizer_HoldCompleted;
    recognizer.HoldCanceled += GestureRecognizer_HoldCanceled;
}
```

>[!NOTE]
>Жесты навигации и манипуляции являются взаимоисключающими в экземпляре *GestureRecognizer*.

### <a name="start-capturing-gestures"></a>Начать запись жестов

По умолчанию *GestureRecognizer* не отслеживает входные данные до тех пор, пока не будет вызван *старткаптурингжестурес ()* . Возможно, что событие жеста может быть создано после вызова *стопкаптурингжестурес ()* , если входные данные были выполнены до того, как был обработан *стопкаптурингжестурес ()* . *GestureRecognizer* будет запоминать, была ли она включена или отключена во время предыдущего кадра, в котором он действительно выполнялся, и поэтому его можно легко запускать и прекращать, исходя из этого кадра.

```cs
recognizer.StartCapturingGestures();
```

### <a name="stop-capturing-gestures"></a>Завершение записи жестов

Чтобы отключить распознавание жестов, сделайте следующее:

```cs
recognizer.StopCapturingGestures();
```

### <a name="removing-a-gesture-recognizer"></a>Удаление распознавателя жестов

Не забудьте отказаться от подписки на подписанные события перед уничтожением объекта *GestureRecognizer* .

```cs
void OnDestroy()
{
    recognizer.Tapped -= GestureRecognizer_Tapped;
    recognizer.HoldStarted -= GestureRecognizer_HoldStarted;
    recognizer.HoldCompleted -= GestureRecognizer_HoldCompleted;
    recognizer.HoldCanceled -= GestureRecognizer_HoldCanceled;
}
```

## <a name="rendering-the-motion-controller-model-in-unity"></a>Подготовка модели контроллера движения к просмотру в Unity

![Модель контроллера движения и телеперенос](images/motioncontrollertest-teleport-1000px.png)<br>
*Модель контроллера движения и телеперенос*

Для подготовки к просмотру в приложении контроллеров движения, соответствующих физическим контроллерам, которые хранятся и обрабатываются при нажатии различных кнопок, можно использовать **prefab мотионконтроллер** в [наборе средств Mixed Reality](https://github.com/Microsoft/MixedRealityToolkit-Unity/).  Этот prefab динамически загружает правильную модель Глтф во время выполнения из установленного в системе драйвера контроллера движения.  Важно динамически загружать эти модели, а не импортировать их вручную в редакторе, чтобы приложение показывало физически точную трехмерную модель для всех существующих и будущих контроллеров, которые могут иметь пользователи.

1. Следуйте инструкциям по [Начало работы](https://github.com/Microsoft/MixedRealityToolkit-Unity/blob/htk_release/GettingStarted.md) , чтобы скачать набор средств для смешанной реальности и добавить его в проект Unity.
2. Если вы заменили камеру на *микседреалитикамерапарент* prefab в рамках начало работы действий, то можете продолжить!  Этот prefab включает визуализацию контроллера движения.  В противном случае добавьте *Assets/холотулкит/input/Prefabs/мотионконтроллерс. prefab* в сцену из области проекта.  Вам потребуется добавить prefab в качестве дочернего объекта для того, чтобы перемещать камеру, когда у пользователя есть телепередачи в пределах сцены, чтобы контроллеры поступали вместе с пользователем.  Если ваше приложение не предусматривает телеперенос, просто добавьте prefab в корень сцены.

## <a name="throwing-objects"></a>Создание вызывающих объектов

Создание объектов в Virtual Reality сложнее, чем может показаться на первый взгляд. Как и в случае с наиболее частыми взаимодействиями, когда вызов в игре происходит непредвиденным образом, он немедленно становится очевидным и нарушается. Мы потратили некоторое время на обдумывание того, как представить физически правильное поведение при вызове и получили несколько рекомендаций по обновлению нашей платформы, которые мы бы хотели поделиться с вами.

Вы можете найти пример того, как мы рекомендуем реализовать [это](https://github.com/keluecke/MixedRealityToolkit-Unity/blob/master/External/Unitypackages/ThrowingStarter.unitypackage)создание. Этот пример соответствует следующим четырем рекомендациям:
* **Используйте *скорость* контроллера вместо расположения**. В ноябре-обновлении Windows мы предоставили изменение в поведении, когда в [позиционированном состоянии отслеживания "приближено](../../design/motion-controllers.md#controller-tracking-state)". В этом состоянии информация о скорости контроллера будет по-прежнему отображаться до тех пор, пока мы считаем высокую точность, которая чаще всего превышает точность.
* **Внедрение *угловой скорости* контроллера**. Эта логика хранится в `throwing.cs` файле в `GetThrownObjectVelAngVel` статическом методе в связанном выше пакете.
   1. По мере того, как угловая скорость сохраняется, создаваемый объект должен поддерживать ту же угловую скорость, что и в момент создания: `objectAngularVelocity = throwingControllerAngularVelocity;`
   2. Поскольку центр массы создаваемого объекта, скорее всего, не находится на происхождении захвата, он, вероятно, имеет разную скорость, чем контроллер в кадре ссылки пользователя. Часть скорости объекта, созданная таким образом, является мгновенно отношения скоростью центра массы создаваемого объекта по отношению к источнику контроллера. Отношения скорость — это перекрестное произведение угловой скорости контроллера с вектором, представляющим расстояние между источником контроллера и центром массы создаваемого объекта.

      ```cs
      Vector3 radialVec = thrownObjectCenterOfMass - throwingControllerPos;
      Vector3 tangentialVelocity = Vector3.Cross(throwingControllerAngularVelocity, radialVec);
      ```

   3. Общая скорость создаваемого объекта — это сумма скорости контроллера и этой отношения скорости: `objectVelocity = throwingControllerVelocity + tangentialVelocity;`

* **Обратите особое внимание на *время* , когда мы применяем скорость**. При нажатии кнопки может пройти до 20 мс, чтобы это событие было переработано с помощью Bluetooth к операционной системе. Это означает, что если вы опрашиваете изменение состояния контроллера с нажатия на не нажимаете или наоборот, сведения об этом контроллере, которые вы получаете вместе с ним, будут действительно впереди этого изменения в состоянии. Кроме того, контроллер, представленный нашим API-интерфейсом опроса, передается в прогноз, чтобы отразить на момент отображения кадра, который может быть более 20 мс в будущем. Это хорошо подходит для *визуализации* удерживаемых объектов, но создает *задачу по времени для объекта* , так как вычисляется траекторию на момент, когда пользователь выпустил исключение. К счастью, при обновлении за Ноябрьское событие при отправке события Unity, такого как *интерактионсаурцепрессед* или *интерактионсаурцерелеасед* , состояние включает данные предыстории с обратной стороны при нажатии или отпускании кнопки.  Чтобы получить наиболее точную визуализацию контроллера и нацеливание контроллера во время возникновения исключения, необходимо правильно использовать опрос и события, если это уместно:
   * Для **отрисовки** каждого кадра на контроллере приложение должно располагать *GameObject* контроллера на переphotonном контроллере в течение текущего кадра.  Эти данные можно получить из API опроса Unity, например *[XR. Инпуттраккинг. Жетлокалпоситион](https://docs.unity3d.com/ScriptReference/XR.InputTracking.GetLocalPosition.html)* или *[XR. Головк. Входные данные. Интерактионманажер. Жеткуррентреадинг](https://docs.unity3d.com/ScriptReference/XR.WSA.Input.InteractionManager.GetCurrentReading.html)*.
   * Для **нацеливания контроллера** на нажатие или выпуск ваше приложение должно райкаст и вычислять траекторий на основе данных о том, что такое событие выпусков.  Эти данные можно получить из API-интерфейсов Unity, таких как *[интерактионманажер. интерактионсаурцепрессед](https://docs.unity3d.com/ScriptReference/XR.WSA.Input.InteractionManager.InteractionSourcePressed.html)*.
* **Использование захвата**. Угловая скорость и скорость отмечаются относительно захвата, а не указателя.

Порождение продолжит совершенствоваться с будущими обновлениями Windows, и вы сможете найти дополнительные сведения здесь.

## <a name="gesture-and-motion-controllers-in-mrtk-v2"></a>Жесты и контроллеры движения в МРТК v2

Вы можете получить доступ к жестам и контроллеру движения из диспетчера ввода.
* [Жест в МРТК v2](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/Input/Gestures.html)
* [Контроллер движения в МРТК v2](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/Input/Controllers.html)


## <a name="follow-along-with-tutorials"></a>Обучение по руководствам

Пошаговые учебники с более подробными примерами настройки доступны в Academyе Mixed Reality:

- [211. Ввод в смешанной реальности: жест](tutorials/holograms-211.md)
- [213. Ввод в смешанной реальности: контроллеры движения](../../deprecated/mixed-reality-213.md)

[![MR-вход 213 — контроллер движения](images/mr213-main-600px.jpg)](https://docs.microsoft.com/windows/mixed-reality/mixed-reality-213)<br>
*MR-вход 213 — контроллер движения*

## <a name="next-development-checkpoint"></a>Следующий этап разработки

Если вы пойдете из пути разработки Unity, мы собрались, что вы в состоянии изучить стандартные блоки МРТК Core. Отсюда вы можете перейти к следующему стандартному блоку:

> [!div class="nextstepaction"]
> [Отслеживание рук и взгляда](hand-eye-in-unit.md)

Или перейдите к возможностям и API платформы смешанной реальности:

> [!div class="nextstepaction"]
> [общие возможности](shared-experiences-in-unity.md);

Вы можете в любой момент вернуться к [этапам разработки для Unity](unity-development-overview.md#2-core-building-blocks).

## <a name="see-also"></a>См. также статью

* [Направление головы и фиксация](../../design/gaze-and-commit.md)
* [Контроллеры движения](../../design/motion-controllers.md)
