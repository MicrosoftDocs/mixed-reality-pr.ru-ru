---
title: HoloLens (1-й общий) и Azure 301 — перевод языка
description: Пройдите этот курс, чтобы узнать, как реализовать API перевода текстов Azure в приложении смешанной реальности.
author: drneil
ms.author: jemccull
ms.date: 07/04/2018
ms.topic: article
keywords: Azure, Mixed Reality, Academy, Unity, учебник, API, текст переводчика, hololens, иммерсивное, VR, языковой перевод, Windows 10, Visual Studio
ms.openlocfilehash: d02b86b6e62a46cd3ed4ebe7e6188cfda18e0d49
ms.sourcegitcommit: 35bd43624be33afdb1bf6ba4ddbe36d268eb9bda
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/20/2021
ms.locfileid: "104730581"
---
# <a name="hololens-1st-gen-and-azure-301-language-translation"></a>HoloLens (1-й общий) и Azure 301: языковой перевод

<br>

>[!NOTE]
>Руководства Mixed Reality Academy были разработаны для иммерсивных гарнитур HoloLens (1-го поколения) и иммерсивных гарнитур Mixed Reality.  Поэтому мы считаем, что важно оставить эти руководства для разработчиков, которые ищут рекомендации по разработке для этих устройств.  Данные руководства **_не_** будут обновляться с учетом последних наборов инструментов или возможностей взаимодействия для HoloLens 2.  Они будут сохранены для работы на поддерживаемых устройствах. Появится новая серия руководств, которые будут опубликованы в будущем, где будет показано, как разрабатывать данные для HoloLens 2.  Это уведомление будет обновлено ссылкой на эти учебники при их публикации.

<br>

В этом курсе вы узнаете, как добавлять возможности перевода в приложение смешанной реальности с помощью Cognitive Services Azure с API перевода текстов.

![Окончательный продукт](images/AzureLabs-Lab1-00.png)

API перевода текстов — это служба перевода, которая работает практически в реальном времени. Служба является облачной, и с помощью REST APIного вызова приложение может использовать технологию нейронного машинного перевода для перевода текста на другой язык. Дополнительные сведения см. на [странице API перевода текстов Azure](https://azure.microsoft.com/services/cognitive-services/translator-text-api/).

После завершения этого курса у вас будет приложение смешанной реальности, которое сможет сделать следующее:

1.  Пользователь будет говорить на микрофон, подключенный к головной гарнитуре (VR) (или встроенному микрофону HoloLens).
2.  Приложение запишет диктовку и отправит ее в API перевода текстов Azure.
3.  Результат перевода будет отображаться в простой группе пользовательского интерфейса в сцене Unity.

В этом курсе вы узнаете, как получить результаты из службы переводчиков в примере приложения на основе Unity. Вы сможете применить эти понятия к настраиваемому приложению, которое вы можете собрать.

## <a name="device-support"></a>Поддержка устройств

<table>
<tr>
<th>Курс</th><th style="width:150px"> <a href="/hololens/hololens1-hardware">HoloLens</a></th><th style="width:150px"> <a href="../../../discover/immersive-headset-hardware-details.md">Иммерсивные гарнитуры</a></th>
</tr><tr>
<td> 301. Смешанная реальность и Azure: перевод с одного языка на другой</td><td style="text-align: center;"> ✔️</td><td style="text-align: center;"> ✔️</td>
</tr>
</table>

> [!NOTE]
> Хотя этот курс в основном ориентирован на гарнитуры Windows Mixed Reality (VR), вы также можете применить сведения, которые вы узнаете в этом курсе, к Microsoft HoloLens. Как вы пройдете вместе с курсом, вы увидите примечания о любых изменениях, которые могут потребоваться для поддержки HoloLens. При использовании HoloLens вы можете заметить некоторые эхо во время записи голоса.

## <a name="prerequisites"></a>Предварительные требования

> [!NOTE]
> Этот учебник предназначен для разработчиков, имеющих базовый опыт работы с Unity и C#. Также имейте в виду, что предварительные требования и письменные инструкции в этом документе отражают, что проверялось и проверено во время написания статьи (Май 2018). Вы можете использовать новейшее программное обеспечение, как указано в статье [Установка средств](../../install-the-tools.md) , но не следует предполагать, что информация в этом курсе будет полностью соответствовать тому, что вы найдете в более новом программном обеспечении, чем показано ниже.

Для этого курса рекомендуется следующее оборудование и программное обеспечение:

- ПК для разработки, [совместимый с Windows Mixed Reality](https://support.microsoft.com/help/4039260/windows-10-mixed-reality-pc-hardware-guidelines) для разработки головных телефонов (VR)
- [Windows 10 для дизайнеров с обновлением (или более поздней версии) с включенным режимом разработчика](../../install-the-tools.md#installation-checklist)
- [Последний пакет SDK для Windows 10](../../install-the-tools.md#installation-checklist)
- [Unity 2017,4](../../install-the-tools.md#installation-checklist)
- [Visual Studio 2017](../../install-the-tools.md#installation-checklist)
- Высокодоступная [гарнитура Windows Mixed Reality (VR)](../../../discover/immersive-headset-hardware-details.md) или [Microsoft HoloLens](/hololens/hololens1-hardware) с включенным режимом разработчика
- Набор наушников со встроенным микрофоном (если у гарнитуры нет встроенного MIC и динамиков);
- Доступ к Интернету для получения сведений о настройке и переводе Azure

## <a name="before-you-start"></a>Перед началом работы

- Чтобы избежать проблем при создании этого проекта, настоятельно рекомендуется создать проект, упомянутый в этом руководстве, в корневой или ближайшем к корневой папке (длинные пути к папкам могут вызвать проблемы во время сборки).
- Код в этом учебнике позволит вам записывать данные с микрофона по умолчанию, подключенного к компьютеру. Убедитесь, что в качестве устройства микрофона по умолчанию выбрано устройство, которое планируется использовать для записи голоса.
- Чтобы разрешить компьютеру Включить диктовку, перейдите в раздел **параметры > конфиденциальность > речь, ввод рукописных данных & ввода** и нажмите кнопку **включить речевые службы и введите предложения**.
- Если вы используете микрофон и наушники, подключенные к гарнитуре (или встроенные в), убедитесь, что параметр "при износе гарнитуры, переключиться на микрофон гарнитуры" включен в **параметрах > смешанной реальности > аудио и речь**.

   ![Параметры смешанной реальности](images/AzureLabs-Lab1-00-5.png)

   ![Настройка микрофона](images/AzureLabs-Lab1-01.png)

> [!WARNING]
> Имейте в виду, что если вы разрабатываете для использования в этой лаборатории экспериментальной гарнитуры, вы можете столкнуться с проблемами устройств вывода звука. Это вызвано проблемой с Unity, которая исправлена в более поздних версиях Unity (Unity 2018,2). Эта ошибка предотвращает изменение устройства выхода звука по умолчанию во время выполнения. Для решения этой проблемы убедитесь, что вы выполнили описанные выше действия, а затем закройте и снова откройте редактор, когда эта проблема будет представлена.

## <a name="chapter-1--the-azure-portal"></a>Глава 1 — портал Azure

Чтобы использовать API-интерфейс Azure Translator, необходимо настроить экземпляр службы, чтобы сделать его доступным для приложения.

1.  Войдите на  [портал Azure](https://portal.azure.com).

    > [!NOTE]
    > Если у вас еще нет учетной записи Azure, необходимо создать ее. Если вы используете этот учебник в учебной или лабораторной ситуации, обратитесь к своему преподавателю или к одной из прокторс, чтобы получить помощь в настройке новой учетной записи.

2.  После входа щелкните **New (создать** ) в левом верхнем углу и выполните поиск по запросу «API перевода текстов». Нажмите клавишу **ВВОД**.

    ![Новый ресурс](images/AzureLabs-Lab1-02.png)

    > [!NOTE]
    > Слово **New** может быть заменено на **создать ресурс** в новых порталах.

3.  На новой странице будет представлено описание службы *API перевода текстов* . В нижнем левом углу этой страницы нажмите кнопку **создать** , чтобы создать связь с этой службой.

    ![Создание службы API перевода текстов](images/AzureLabs-Lab1-03.png)

4.  После нажатия кнопки **создать**:

    1. Вставьте нужное **имя** для этого экземпляра службы.
    2. Выберите подходящую **подписку**.
    3. Выберите **ценовую категорию** , подходящую для вас. Если вы впервые создаете *службу перевод текстов*, вам будет доступен бесплатный уровень (с именем F0).
    4. Выберите **группу ресурсов** или создайте новую. Группа ресурсов предоставляет способ мониторинга, контроля доступа, подготовки счетов и управления ими для коллекции ресурсов Azure. Рекомендуется, чтобы все службы Azure, связанные с одним проектом (например, в этих лабораториях), были в общей группе ресурсов.

        > Если вы хотите ознакомиться с дополнительными сведениями о группах ресурсов Azure, обратитесь [к статье о группе ресурсов](/azure/azure-resource-manager/resource-group-portal).

    5. Определите **Расположение** группы ресурсов (при создании новой группы ресурсов). В идеале это расположение будет находиться в регионе, в котором будет выполняться приложение. Некоторые ресурсы Azure доступны только в определенных регионах.
    6. Также необходимо подтвердить, что вы поняли условия, примененные к этой службе.
    7. Нажмите кнопку **создания**.

        ![Нажмите кнопку Создать.](images/AzureLabs-Lab1-04.png)

5.  После нажатия кнопки **создать** необходимо подождать, пока не будет создана служба, а это может занять некоторое время.
6.  После создания экземпляра службы на портале отобразится уведомление. 

    ![Уведомление о создании службы Azure](images/AzureLabs-Lab1-05.png)

7.  Щелкните уведомление, чтобы просмотреть новый экземпляр службы. 

    ![Перейдите к всплывающему окну ресурсов.](images/AzureLabs-Lab1-06.png)

8.  Нажмите кнопку " **Переход к ресурсу** " в уведомлении, чтобы изучить новый экземпляр службы. Вы будете перенаправлены на новый экземпляр службы API перевода текстов. 

    ![Страница службы API перевода текстов](images/AzureLabs-Lab1-07.png)

9.  В рамках этого руководства приложение должно будет вызывать службу, что выполняется с помощью ключа подписки вашей службы. 
10. На странице *быстрого запуска* службы *перевод текстов* перейдите к первому шагу, *Возьмите ключи* и щелкните **ключи** (это можно также сделать, щелкнув синие клавиши-гиперссылки, расположенные в меню навигации службы, обозначенном значком ключа). При этом будут раскрыты *ключи* службы.
11. Сделайте копию одного из отображаемых ключей, так как это потребуется позже в проекте. 

## <a name="chapter-2--set-up-the-unity-project"></a>Глава 2 — Настройка проекта Unity

Настройка и тестирование иммерсивного наушников смешанной реальности.

> [!NOTE]
> Для этого курса не потребуется контроллеры движения. Если вам нужна поддержка настройки иммерсивного головного телефона, выполните следующие [действия](https://support.microsoft.com/help/4043101/windows-10-set-up-windows-mixed-reality).

Ниже приведена типичная Настройка для разработки с использованием смешанной реальности и, как таковая, является хорошим шаблоном для других проектов:

1.  Откройте *Unity* и нажмите кнопку **создать**. 

    ![Запуск нового проекта Unity.](images/AzureLabs-Lab1-08.png)

2.  Теперь необходимо указать имя проекта Unity. Вставка **MR_Translation**. Убедитесь, что для типа проекта задано значение **3D**. Задайте для *расположения нужное расположение* (Помните, что ближе к корневым каталогам лучше). Затем нажмите кнопку **создать проект**.

    ![Укажите сведения о новом проекте Unity.](images/AzureLabs-Lab1-09.png)

3.  При открытом Unity стоит проверить, что для **редактора скриптов** по умолчанию задано значение **Visual Studio**. Перейдите к разделу **изменение параметров >** , а затем в новом окне перейдите к разделу **Внешние инструменты**. Измените **Редактор внешних скриптов** на **Visual Studio 2017**. Закройте окно **настройки** .

    ![Обновить настройки редактора скриптов.](images/AzureLabs-Lab1-10.png)

4.  Затем перейдите в раздел **файл > параметры сборки** и переключите платформу на **универсальная платформа Windows**, нажав кнопку **коммутатора платформы** .

    ![Окно "параметры сборки". Переключите платформу в UWP.](images/AzureLabs-Lab1-11.png)

5.  Перейдите в раздел **файл > параметры сборки** и убедитесь в том, что:

    1. **Целевое устройство** настроено для **любого устройства**.

        > Для Microsoft HoloLens задайте для параметра **целевое устройство** значение *HoloLens*.

    2. Для **типа сборки** задано значение **D3D**
    3. **Пакет SDK** установлен в значение " **Последняя установка** "
    4. Для **версии Visual Studio** установлено значение " **Последняя установка** "
    5. **Сборка и запуск** настроены на **локальный компьютер**
    6. Сохраните сцену и добавьте ее в сборку.

        1. Для этого выберите **Добавить открытые сцены**. Появится окно сохранения.

            ![Нажмите кнопку Добавить кнопку "открыть сцены"](images/AzureLabs-Lab1-12.png)

        2. Создайте новую папку для этого, а также любой будущей сцены, а затем нажмите кнопку **создать папку** , чтобы создать новую папку, назовите ее « **сцены**».

            ![Создать новую папку скриптов](images/AzureLabs-Lab1-13.png)

        3. Откройте только что созданную папку **сцены** , а затем в поле *имя файла*: введите **MR_TranslationScene**, а затем нажмите кнопку **сохранить**.

            ![Присвойте имя новой сцене.](images/AzureLabs-Lab1-14.png)

            > Помните, что сцены Unity необходимо сохранять в папке *Assets* , так как они должны быть связаны с проектом Unity. Создание папки «сцены» (и других аналогичных папок) — типичный способ структуризации проекта Unity.

    7. Оставшиеся параметры, в *параметрах сборки*, должны быть оставлены по умолчанию.

6. В окне *параметры сборки* нажмите кнопку **Параметры проигрывателя** , чтобы открыть связанную панель в пространстве, где находится *инспектор* . 

    ![Откройте параметры проигрывателя.](images/AzureLabs-Lab1-15.png)

7. На этой панели необходимо проверить несколько параметров:

    1. На вкладке **другие параметры** выполните следующие действия.

        1. **Версия среды выполнения сценариев** должна быть **стабильной** (эквивалент .NET 3,5).
        2. **Серверная часть сценариев** должна быть **.NET**
        3. **Уровень совместимости API** должен быть **.NET 4,6**

            ![Обновите другие параметры.](images/AzureLabs-Lab1-16.png)
      
    2. На вкладке **Параметры публикации** в разделе **возможности** установите флажок:

        1. **InternetClient**;
        2. **Микрофон**

            ![Обновляются параметры публикации.](images/AzureLabs-Lab1-17.png)

    3. На более низких панели в **параметрах XR** (см. ниже **Параметры публикации**), **поддерживаемая виртуальная реальность** Tick, убедитесь, что добавлен **пакет SDK для Windows Mixed Reality** .

        ![Обновите параметры X R.](images/AzureLabs-Lab1-18.png)

8.  Вернемся к **параметрам сборки**. *проекты C# для Unity* больше не заключаются; Установите флажок рядом с этим. 
9.  Закройте окно Build Settings (Параметры сборки).
10. Сохраните сцену и проект (**файл > сохранить сцену или файл > сохранить проект**).

## <a name="chapter-3--main-camera-setup"></a>Глава 3 — Настройка основной камеры

> [!IMPORTANT]
> Если вы хотите пропустить компонент *установки Unity, установленный* в этом курсе, и продолжить работу с кодом, [Скачайте этот файл. пакет unitypackage](https://github.com/Microsoft/HolographicAcademy/raw/Azure-MixedReality-Labs/Azure%20Mixed%20Reality%20Labs/MR%20and%20Azure%20301%20-%20Language%20translation/Azure-MR-301.unitypackage), импортируйте его в проект как [*пользовательский пакет*](https://docs.unity3d.com/Manual/AssetPackages.html), а затем продолжайте в [главе 5](#chapter-5--create-the-results-class). Вам все равно придется создать проект Unity.

1.  На *панели Иерархия* вы найдете объект с названием **Главная камера**, этот объект представляет "головную точку представления", когда вы "внутри приложения".
2.  На панели мониторинга Unity перед вами выберите **основную камеру GameObject**. Вы заметите, что на *панели инспектора* (как правило, на панели мониторинга) отображаются различные компоненты этого *GameObject*, с помощью кнопки *преобразовывать* в верхней части, за которой следует *Камера* и некоторые другие компоненты. Необходимо будет сбросить преобразование основной камеры, чтобы она правильно расположиться.
3.  Для этого щелкните значок **шестеренки** рядом с компонентом *преобразования* камеры и выберите **Сброс**. 

    ![Сброс основного преобразования камеры.](images/AzureLabs-Lab1-19.png)
 
4.  Компонент *преобразования* должен выглядеть следующим образом:

    1. Для этой *должности* задано значение **0, 0, 0**
    2. Для *вращения* задано значение **0, 0, 0**
    3. Для параметра *Scale* устанавливается значение **1, 1, 1** .

        ![Сведения о преобразовании для камеры](images/AzureLabs-Lab1-20.png)

5.  Затем, выбрав **основной объект Camera** , ознакомьтесь с разрядом с кнопкой **Добавить компонент** , расположенной в самом низу *панели инспектора*. 
6.  Выберите эту кнопку и выполните поиск (для этого введите " *аудио-источник* " в поле поиска или перейдите к разделам) для компонента с названием " **источник звука** ", как показано ниже, и выберите его (нажмите клавишу ВВОД на нем).
7.  Компонент " *звуковый источник* " будет добавлен на **основную камеру**, как показано ниже.

    ![Добавьте компонент "источник звука".](images/AzureLabs-Lab1-21.png)

    > [!NOTE]
    > Для Microsoft HoloLens необходимо также изменить следующие компоненты, которые являются частью компонента **камеры** на **основной камере**:
    > - **Снять флаги:** Сплошной цвет.
    > - **Фоновый режим** "Черный, альфа 0" — шестнадцатеричный цвет: #00000000.

## <a name="chapter-4--setup-debug-canvas"></a>Глава 4 — настройка отладочного холста

Чтобы отобразить входные и выходные данные перевода, необходимо создать базовый пользовательский интерфейс. В этом курсе вы создадите объект пользовательского интерфейса Canvas с несколькими объектами "Text" для отображения данных.

1.  Щелкните правой кнопкой мыши пустую область *панели Иерархия*, в разделе **Пользовательский интерфейс** добавьте **холст**.

    ![Добавить новый объект пользовательского интерфейса Canvas.](images/AzureLabs-Lab1-22.png)

2.  Выбрав объект Canvas, на *панели инспектора* (в компоненте Canvas) измените **режим рендеринга** на **мировое пространство**. 
3.  Затем измените следующие параметры в *преобразовании Rect на панели инспектора*:

    1. *POS-терминал*  -   **X** 0 **Y** 0 **Z** 40
    2. *Ширина* — 500
    3. *Высота* 300
    4. *Масштаб*  -  **X** 0,13 **Y** 0,13 **Z** 0,13

        ![Обновите преобразование Rect для холста.](images/AzureLabs-Lab1-23.png)
 
4.  Щелкните правой кнопкой мыши **холст** на *панели Иерархия*, в разделе **Пользовательский интерфейс** и добавьте **панель**. Эта **панель** предоставит фон текста, который будет отображаться в сцене.
5.  Щелкните правой кнопкой мыши **панель** на *панели Иерархия*, в разделе **Пользовательский интерфейс** и добавьте **текстовый объект**. Повторяйте тот же процесс, пока вы не создадите четыре текстовых объекта пользовательского интерфейса в целом (Подсказка: если у вас есть первый объект "Text", можно просто нажать клавишу **"Ctrl" + "+ 'd"**, чтобы продублировать его, пока не будет всего четыре). 
6.  Для каждого **текстового объекта** выберите его и используйте приведенные ниже таблицы, чтобы задать параметры на *панели инспектора*.

    1. Для компонента *преобразования Rect* :

        | Имя                   | Transform- *позиционирование*             | Ширина      | Высота:    |
        |:----------------------:|:----------------------------------:|:----------:|:---------:|
        | микрофонестатуслабел  | **X** -80 **Y** 90 **Z** 0         | 300        | 30        |
        | азуререспонселабел     | **X** -80 **Y** 30 **Z** 0         | 300        | 30        |
        | диктатионлабел         | **X** -80 **Y** – 30 **Z** 0        | 300        | 30        |
        | транслатионресултлабел | **X** -80 **Y** -90 **Z** 0        | 300        | 30        |


    2. Для компонента **Text (script)** :


        | Имя                   | Текст               | Размер шрифта    |
        |:----------------------:|:------------------:|:------------:|
        | микрофонестатуслабел  | Состояние микрофона: | 20           |
        | азуререспонселабел     | Веб-ответ Azure | 20           |
        | диктатионлабел         |   Вы только что сказали:   | 20           |
        | транслатионресултлабел |    Перевод:    | 20           |

        ![Введите соответствующие значения для меток пользовательского интерфейса.](images/AzureLabs-Lab1-24.png)

    3. Кроме того, сделайте шрифт **полужирным**. Это упростит чтение текста.

        ![Полужирный шрифт.](images/AzureLabs-Lab1-25.png)

7.  Для каждого *объекта текста пользовательского интерфейса* , созданного в [главе 5](#chapter-5--create-the-results-class), создайте новый *дочерний* **текстовый объект пользовательского интерфейса**. Эти дочерние элементы будут отображать выходные данные приложения. Создайте *дочерние* объекты, щелкнув правой кнопкой мыши предполагаемый родительский объект (например, *микрофонестатуслабел*), а затем выберите пункт **Пользовательский интерфейс** , а затем выберите пункт **текст**.
8.  Для каждого из этих дочерних элементов выберите его и используйте приведенные ниже таблицы, чтобы задать параметры на панели инспектора.

    1. Для компонента **преобразования Rect** :

        | Имя                  | Transform- *позиционирование* | Ширина      | Высота:    |
        |:---------------------:|:----------------------:|:----------:|:---------:|
        | микрофонестатустекст  | X 0 Y – 30 Z 0          | 300        | 30        |
        | азуререспонсетекст     | X 0 Y – 30 Z 0          | 300        | 30        |
        | диктатионтекст         | X 0 Y – 30 Z 0          | 300        | 30        |
        | транслатионресулттекст | X 0 Y – 30 Z 0          | 300        | 30        |

    2. Для компонента **Text (script)** :

        | Имя                  | Текст          | Размер шрифта    |
        |:---------------------:|:-------------:|:------------:|
        | микрофонестатустекст  |      ??       | 20           |
        | азуререспонсетекст     |      ??       | 20           |
        | диктатионтекст         |      ??       | 20           |
        | транслатионресулттекст |      ??       | 20           |

9. Затем выберите параметр выравнивания "центр" для каждого текстового компонента:

    ![Выровняйте текст.](images/AzureLabs-Lab1-26.png)

10. Чтобы обеспечить простоту чтения **дочерних объектов пользовательского интерфейса** , измените их *Цвет*. Для этого щелкните полосу (в настоящий момент черная) рядом с пунктом *Цвет*. 

    ![Введите соответствующие значения для вывода текста пользовательского интерфейса.](images/AzureLabs-Lab1-27.png)
 
11. Затем в окне новое, небольшое *цветовое* значение измените *шестнадцатеричный цвет* на: **0032EAFF** .

    ![Измените цвет на синий.](images/AzureLabs-Lab1-28.png)
 
12. Ниже показано, как должен выглядеть **Пользовательский интерфейс** .
    1.  На *панели Иерархия*:

        ![Иметь иерархию в предоставленной структуре.](images/AzureLabs-Lab1-29.png)

    2.  В *представлениях* *сцены* и игры:

        ![Отображение сцен и игровых представлений в одной и той же структуре.](images/AzureLabs-Lab1-30.png)

## <a name="chapter-5--create-the-results-class"></a>Глава 5 — Создание класса Results

Первый скрипт, который необходимо создать, — это класс *Results* , который отвечает за предоставление способа просмотра результатов перевода. Класс сохраняет и отображает следующее: 

- Результат ответа из Azure.
- Состояние микрофона. 
- Результат диктовки (Voice to Text).
- Результат перевода.

Чтобы создать этот класс, сделайте следующее: 

1.  Щелкните правой кнопкой мыши на *панели проект*, а затем **Создайте > папку**. Назовите папку **Scripts**. 
 
    ![Создайте папку Scripts.](images/AzureLabs-Lab1-31.png)

    ![Откройте папку «скрипты».](images/AzureLabs-Lab1-32.png)
 
2.  Создав папку **Scripts** , дважды щелкните ее, чтобы открыть. Затем в этой папке щелкните правой кнопкой мыши и выберите **создать >** затем **скрипт C#**. Назовите *результаты* скрипта. 

    ![Создайте первый скрипт.](images/AzureLabs-Lab1-33.png)
 
3.  Дважды щелкните новый скрипт *Results* , чтобы открыть его в **Visual Studio**.
4.  Вставьте следующие пространства имен:

    ```cs
        using UnityEngine;
        using UnityEngine.UI;
    ```

5.  Внутри класса вставьте следующие переменные:

    ```cs
        public static Results instance;
   
        [HideInInspector] 
        public string azureResponseCode;
   
        [HideInInspector] 
        public string translationResult;
   
        [HideInInspector] 
        public string dictationResult;
   
        [HideInInspector] 
        public string micStatus;
   
        public Text microphoneStatusText;
   
        public Text azureResponseText;
   
        public Text dictationText;
   
        public Text translationResultText;
    ```

6.  Затем добавьте метод " *спящий ()* ", который будет вызываться при инициализации класса. 

    ```csharp
        private void Awake() 
        { 
            // Set this class to behave similar to singleton 
            instance = this;           
        } 
    ```

7.  Наконец, добавьте методы, которые отвечают за вывод различных сведений о результатах в пользовательский интерфейс. 

    ```csharp
        /// <summary>
        /// Stores the Azure response value in the static instance of Result class.
        /// </summary>
        public void SetAzureResponse(string result)
        {
            azureResponseCode = result;
            azureResponseText.text = azureResponseCode;
        }
   
        /// <summary>
        /// Stores the translated result from dictation in the static instance of Result class. 
        /// </summary>
        public void SetDictationResult(string result)
        {
            dictationResult = result;
            dictationText.text = dictationResult;
        }
   
        /// <summary>
        /// Stores the translated result from Azure Service in the static instance of Result class. 
        /// </summary>
        public void SetTranslatedResult(string result)
        {
            translationResult = result;
            translationResultText.text = translationResult;
        }
   
        /// <summary>
        /// Stores the status of the Microphone in the static instance of Result class. 
        /// </summary>
        public void SetMicrophoneStatus(string result)
        {
            micStatus = result;
            microphoneStatusText.text = micStatus;
        }
    ```

8.  Не забудьте сохранить изменения в *Visual Studio* перед возвратом в *Unity*.

## <a name="chapter-6--create-the-microphonemanager-class"></a>Глава 6 — создание класса *микрофонеманажер*

Второй класс, который предстоит создать, — это *микрофонеманажер*.

Этот класс отвечает за:

- Обнаружение устройства записи, подключенного к гарнитуре или компьютеру (в зависимости от значения по умолчанию).
- Запишите звук (голосовой) и используйте диктовку, чтобы сохранить ее в виде строки.
- После приостановки голоса отправьте диктовку в класс Translator.
- Разместите метод, который может при необходимости прекращать запись речи.

Чтобы создать этот класс, сделайте следующее: 
1.  Дважды щелкните папку **Scripts** , чтобы открыть ее. 
2.  Щелкните правой кнопкой мыши в папке **Scripts** и выберите **создать > скрипт C#**. Назовите сценарий *микрофонеманажер*. 
3.  Дважды щелкните новый скрипт, чтобы открыть его в Visual Studio.
4.  Обновите пространства имен, как показано ниже, в верхней части класса *микрофонеманажер* :

    ```csharp
        using UnityEngine; 
        using UnityEngine.Windows.Speech;
    ```

5.  Затем добавьте следующие переменные в класс *микрофонеманажер* :

    ```csharp
        // Help to access instance of this object 
        public static MicrophoneManager instance; 
     
        // AudioSource component, provides access to mic 
        private AudioSource audioSource; 
   
        // Flag indicating mic detection 
        private bool microphoneDetected; 
   
        // Component converting speech to text 
        private DictationRecognizer dictationRecognizer; 
    ```

6.  Теперь необходимо добавить код для методов *спящего режима ()* и *Start ()* . Они будут вызываться при инициализации класса:

    ```csharp
        private void Awake() 
        { 
            // Set this class to behave similar to singleton 
            instance = this; 
        } 
    
        void Start() 
        { 
            //Use Unity Microphone class to detect devices and setup AudioSource 
            if(Microphone.devices.Length > 0) 
            { 
                Results.instance.SetMicrophoneStatus("Initialising..."); 
                audioSource = GetComponent<AudioSource>(); 
                microphoneDetected = true; 
            } 
            else 
            { 
                Results.instance.SetMicrophoneStatus("No Microphone detected"); 
            } 
        } 
    ```

7.  Метод *Update ()* можно *Удалить* , так как этот класс не будет его использовать.
8.  Теперь вам потребуются методы, которые приложение использует для запуска и завершения записи речи, и передайте его классу *Translator* , который вы будете создавать в ближайшее время. Скопируйте приведенный ниже код и вставьте его под методом *Start ()* .

    ```csharp
        /// <summary> 
        /// Start microphone capture. Debugging message is delivered to the Results class. 
        /// </summary> 
        public void StartCapturingAudio() 
        { 
            if(microphoneDetected) 
            {               
                // Start dictation 
                dictationRecognizer = new DictationRecognizer(); 
                dictationRecognizer.DictationResult += DictationRecognizer_DictationResult; 
                dictationRecognizer.Start(); 
    
                // Update UI with mic status 
                Results.instance.SetMicrophoneStatus("Capturing..."); 
            }      
        } 
 
        /// <summary> 
        /// Stop microphone capture. Debugging message is delivered to the Results class. 
        /// </summary> 
        public void StopCapturingAudio() 
        { 
            Results.instance.SetMicrophoneStatus("Mic sleeping"); 
            Microphone.End(null); 
            dictationRecognizer.DictationResult -= DictationRecognizer_DictationResult; 
            dictationRecognizer.Dispose(); 
        }
    ```

    > [!TIP]
    > Несмотря на то, что это приложение не будет использовать его, в этой статье также приведен метод *стопкаптурингаудио ()* , который позволяет реализовать возможность отмены записи звука в приложении.

9.  Теперь необходимо добавить обработчик диктовки, который будет вызываться при остановке голоса. Затем этот метод передает продиктованный текст классу *Translator* .

    ```csharp
        /// <summary>
        /// This handler is called every time the Dictation detects a pause in the speech. 
        /// Debugging message is delivered to the Results class.
        /// </summary>
        private void DictationRecognizer_DictationResult(string text, ConfidenceLevel confidence)
        {
            // Update UI with dictation captured
            Results.instance.SetDictationResult(text);
   
            // Start the coroutine that process the dictation through Azure 
            StartCoroutine(Translator.instance.TranslateWithUnityNetworking(text));   
        }
    ```

10. Не забудьте сохранить изменения в Visual Studio перед возвратом в Unity.

> [!WARNING]  
> На этом этапе вы увидите ошибку на панели *консоли редактора Unity* ("имя" Translator "не существует..."). Это обусловлено тем, что код ссылается на класс *Translator* , который будет создан в следующей главе.

## <a name="chapter-7--call-to-azure-and-translator-service"></a>Глава 7 — вызов службы Azure и переводчика

Последний скрипт, который необходимо создать, — это класс *Translator* . 

Этот класс отвечает за:

-   Проверка подлинности приложения в *Azure* в Exchange для **токена проверки** подлинности.
-   Используйте **токен проверки подлинности** для передачи текста (полученного от класса *микрофонеманажер* ) для перевода.
-   Получение переведенного результата и передача его в класс *Results* для визуализации в пользовательском интерфейсе.

Чтобы создать этот класс, сделайте следующее: 
1.  Перейдите к созданной ранее папке **Scripts** . 
2.  Щелкните правой кнопкой мыши на **панели проект**, **Создайте > скрипт C#**. Вызовите *транслятор* скриптов.
3.  Дважды щелкните новый скрипт *переводчика* , чтобы открыть его в **Visual Studio**.
4.  Добавьте следующие пространства имен в начало файла:

    ```csharp
        using System;
        using System.Collections;
        using System.Xml.Linq;
        using UnityEngine;
        using UnityEngine.Networking;
    ```

5.  Затем добавьте следующие переменные в класс *Translator* :

    ```csharp
        public static Translator instance; 
        private string translationTokenEndpoint = "https://api.cognitive.microsoft.com/sts/v1.0/issueToken"; 
        private string translationTextEndpoint = "https://api.microsofttranslator.com/v2/http.svc/Translate?"; 
        private const string ocpApimSubscriptionKeyHeader = "Ocp-Apim-Subscription-Key"; 
    
        //Substitute the value of authorizationKey with your own Key 
        private const string authorizationKey = "-InsertYourAuthKeyHere-"; 
        private string authorizationToken; 
    
        // languages set below are: 
        // English 
        // French 
        // Italian 
        // Japanese 
        // Korean 
        public enum Languages { en, fr, it, ja, ko }; 
        public Languages from = Languages.en; 
        public Languages to = Languages.it; 
    ```

    > [!NOTE]
    > - Языки, вставленные в **перечисление** языков, являются просто примерами. Вы можете добавить дополнительные сведения, если хотите. [API поддерживает более 60 языков](/azure/cognitive-services/translator/languages) (включая марсианского)!
    > - Существует [более интерактивная страница, охватывающая доступные языки](https://www.microsoft.com/translator/business/languages/), хотя имейте в виду, что страница работает только в том случае, если для языка сайта задано значение "" (а веб-сайт Майкрософт, скорее всего, перенаправляется на ваш собственный язык). Язык сайта можно изменить в нижней части страницы или путем изменения URL-адреса.
    > - Значение **authorizationkey согласно инструкциям** в приведенном выше фрагменте кода должно быть **ключом**  , полученным при оформлении подписки на *API перевода текстов Azure*. Это было рассмотрено в [главе 1](#chapter-1--the-azure-portal).

6.  Теперь необходимо добавить код для методов *спящего режима ()* и *Start ()* . 
7.  В этом случае код выполнит вызов *Azure* с помощью ключа авторизации, чтобы получить *маркер*.

    ```csharp
        private void Awake() 
        { 
            // Set this class to behave similar to singleton  
            instance = this; 
        } 
    
        // Use this for initialization  
        void Start() 
        { 
            // When the application starts, request an auth token 
            StartCoroutine("GetTokenCoroutine", authorizationKey); 
        }
    ```

    > [!NOTE]
    > Срок действия маркера истечет через 10 минут. В зависимости от сценария для приложения может потребоваться несколько раз выполнять один и тот же соподпрограммный вызов.

8.  Для получения маркера используется следующая соподпрограмма:

    ```csharp
        /// <summary> 
        /// Request a Token from Azure Translation Service by providing the access key. 
        /// Debugging result is delivered to the Results class. 
        /// </summary> 
        private IEnumerator GetTokenCoroutine(string key)
        {
            if (string.IsNullOrEmpty(key))
            {
                throw new InvalidOperationException("Authorization key not set.");
            }

            using (UnityWebRequest unityWebRequest = UnityWebRequest.Post(translationTokenEndpoint, string.Empty))
            {
                unityWebRequest.SetRequestHeader("Ocp-Apim-Subscription-Key", key);
                yield return unityWebRequest.SendWebRequest();

                long responseCode = unityWebRequest.responseCode;

                // Update the UI with the response code 
                Results.instance.SetAzureResponse(responseCode.ToString());

                if (unityWebRequest.isNetworkError || unityWebRequest.isHttpError)
                {
                    Results.instance.azureResponseText.text = unityWebRequest.error;
                    yield return null;
                }
                else
                {
                    authorizationToken = unityWebRequest.downloadHandler.text;
                }
            }

            // After receiving the token, begin capturing Audio with the MicrophoneManager Class 
            MicrophoneManager.instance.StartCapturingAudio();
        }
    ```

    > [!WARNING]
    > При изменении имени метода IEnumerator **жеттокенкораутине ()** необходимо обновить значения строк вызова *старткораутине* и *стопкораутине* в приведенном выше коде. Как и в случае с [документацией Unity](https://docs.unity3d.com/ScriptReference/MonoBehaviour.StartCoroutine.html), для отмены конкретной *соподпрограммы* необходимо использовать метод строкового значения.

9.  Затем добавьте соподпрограмму (с помощью метода потока поддержки прямо под ним), чтобы получить перевод текста, полученного классом *микрофонеманажер* . Этот код создает строку запроса для отправки в *API перевода текстов Azure*, а затем использует внутренний класс Unity унитивебрекуест для выполнения вызова Get к конечной точке со строкой запроса. Затем результат используется для задания перевода в объекте Results. В приведенном ниже коде показана реализация:

    ```csharp
        /// <summary> 
        /// Request a translation from Azure Translation Service by providing a string.  
        /// Debugging result is delivered to the Results class. 
        /// </summary> 
        public IEnumerator TranslateWithUnityNetworking(string text)
        {
            // This query string will contain the parameters for the translation 
            string queryString = string.Concat("text=", Uri.EscapeDataString(text), "&from=", from, "&to=", to);

            using (UnityWebRequest unityWebRequest = UnityWebRequest.Get(translationTextEndpoint + queryString))
            {
                unityWebRequest.SetRequestHeader("Authorization", "Bearer " + authorizationToken);
                unityWebRequest.SetRequestHeader("Accept", "application/xml");
                yield return unityWebRequest.SendWebRequest();

                if (unityWebRequest.isNetworkError || unityWebRequest.isHttpError)
                {
                    Debug.Log(unityWebRequest.error);
                    yield return null;
                }

                // Parse out the response text from the returned Xml
                string result = XElement.Parse(unityWebRequest.downloadHandler.text).Value;
                Results.instance.SetTranslatedResult(result);
            }
        }
    ```

10. Не забудьте сохранить изменения в *Visual Studio* перед возвратом в *Unity*.

## <a name="chapter-8--configure-the-unity-scene"></a>Глава 8 — Настройка сцены Unity

1.  В редакторе Unity щелкните и перетащите класс *Results* *из* папки **сценарии** в объект **Main** на *панели Иерархия*.
2.  Щелкните **основную камеру** и посмотрите на *Панель инспектора*. Вы заметите, что в вновь добавленном компоненте *скрипта* есть четыре поля с пустыми значениями. Это выходные ссылки на свойства в коде. 
3.  Перетащите соответствующие **текстовые** объекты с *панели Иерархия* в эти четыре слота, как показано на рисунке ниже.

    ![Обновить ссылки на целевые объекты указанными значениями.](images/AzureLabs-Lab1-34.png)
  
4.  Затем перетащите класс *Translator* из папки **Scripts** в **главный объект Camera** на *панели Иерархия*. 
5.  Затем перетащите класс *микрофонеманажер* из папки **Scripts** в **основной объект Camera** на *панели Иерархия*. 
6.  Наконец, щелкните **основную камеру** и посмотрите на *Панель инспектора*. Вы заметите, что в скрипте, который вы перетащили, есть два раскрывающихся списка, которые позволяют задать языки.
 
    ![Убедитесь, что нужные языки перевода введены.](images/AzureLabs-Lab1-35.png)

## <a name="chapter-9--test-in-mixed-reality"></a>Глава 9 — тестирование в смешанной реальности

На этом этапе необходимо проверить, правильно ли реализована сцена.

Убедитесь в следующем:

- Все параметры, упомянутые в [главе 1](#chapter-1--the-azure-portal) , заданы правильно. 
- *Результаты*, *переводчики* и *микрофонеманажер* присоединяются к **главному объекту Camera** . 
- Вы поместили **ключ** службы *API перевода текстов Azure* в переменную **authorizationkey согласно инструкциям** в скрипте *переводчика* .  
- Все поля на *панели инспектора основной камеры* назначены должным образом.
- Микрофон работает при запуске сцены (если нет, убедитесь, что подключенный микрофон является устройством *по умолчанию* и что вы [правильно настроили его в Windows](https://support.microsoft.com/help/4027981/windows-how-to-set-up-and-test-microphones-in-windows-10)).

Можно протестировать иммерсивное гарнитуру, нажав кнопку **воспроизвести** в *редакторе Unity*.
Приложение должно работать через присоединенную иммерсивное гарнитуру.

> [!WARNING]  
> Если в консоли Unity появляется сообщение об ошибке о смене звукового устройства по умолчанию, возможно, сцена не работает должным образом. Это связано с тем, как портал смешанной реальности работает со встроенными микротелефонами для гарнитур, имеющих их. Если вы видите эту ошибку, просто закройте сцену и снова запустите ее, и она должна работать должным образом.

## <a name="chapter-10--build-the-uwp-solution-and-sideload-on-local-machine"></a>Глава 10 — Создание решения UWP и загружать неопубликованные на локальном компьютере

Все необходимое для раздела Unity этого проекта теперь завершено, поэтому пришло время создать его из Unity.

1.  Выберите **параметры сборки**: **файл > параметры сборки...**
2.  В окне " **параметры сборки** " щелкните **Сборка**.

    ![Создание сцены Unity.](images/AzureLabs-Lab1-36.png)
  
3.  Если это еще не так, то **проект Tick Unity C#**.
4.  Щелкните **Построить**. Unity запустит окно *проводника* , в котором необходимо создать, а затем выбрать папку для сборки приложения. Создайте эту папку сейчас и назовите ее *app* Name. Затем выберите папку *приложения* и нажмите кнопку **выбрать папку**. 
5.  Unity начнет сборку проекта в папку *приложения* . 
6.  После того как Unity завершит сборку (может занять некоторое время), он откроет окно *проводника* в расположении сборки (проверьте панель задач, так как она может не всегда отображаться над окнами, но будет уведомлять о добавлении нового окна).

## <a name="chapter-11--deploy-your-application"></a>Глава 11 — развертывание приложения

Чтобы развернуть приложение, выполните следующие действия.

1.  Перейдите к новой сборке Unity (папка *приложения* ) и откройте файл решения в *Visual Studio*.
2.  В конфигурации решения выберите **Отладка**.
3.  На платформе решения выберите **x86**, **локальный компьютер**. 

    > Для Microsoft HoloLens может быть проще установить этот параметр на *Удаленный компьютер*, чтобы вы не были подключены к компьютеру. Однако необходимо также выполнить следующие действия.
    > - Определите **IP-адрес** HoloLens, который можно найти в *параметрах > сети & интернет > Wi-Fi > дополнительные параметры*. IPv4 — это адрес, который следует использовать. 
    > - Убедитесь, **что включен** *режим разработчика* ; находится в *параметрах > Update & > безопасности для разработчиков*.

    ![Разверните решение из Visual Studio.](images/AzureLabs-Lab1-37.png)
    
 
4.  Перейдите в **меню "сборка** " и щелкните " **Развернуть решение** ", чтобы загружать неопубликованные приложение на компьютер.
5.  Теперь приложение должно отобразиться в списке установленных приложений, готовых к запуску.
6.  После запуска приложение предложит авторизовать доступ к микрофону. Обязательно нажмите кнопку « **Да** ».
7.  Теперь вы готовы начать трансляцию!

## <a name="your-finished-translation-text-api-application"></a>Законченное приложение API для перевода текста

Поздравляем! вы создали приложение смешанной реальности, которое использует API перевода текста Azure для преобразования речи в переведенный текст.

![Окончательный продукт.](images/AzureLabs-Lab1-00.png)

## <a name="bonus-exercises"></a>Дополнительные упражнения

### <a name="exercise-1"></a>Упражнение 1

Можно ли добавить в приложение функции преобразования текста в речь, чтобы возвращаемый текст был произнесен?

### <a name="exercise-2"></a>Упражнение 2

Предоставление пользователю возможности изменять языки исходного и выходного кода ("с" и "на") в самом приложении, поэтому не требуется перестраивать приложение каждый раз, когда необходимо изменить языки.