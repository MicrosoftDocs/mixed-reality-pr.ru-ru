---
title: Выбор целевого объекта отслеживания взгляда
description: Как получить доступ к данным и отдельным событиям взгляда на глаза для выбора целевых объектов в МРТК
author: CDiaz-MS
ms.author: cadia
ms.date: 01/12/2021
keywords: Unity, HoloLens, HoloLens 2, смешанная реальность, разработка, мртк, эйетраккинг,
ms.openlocfilehash: aab2f35259db183f4f3edb4fffc2b3e7a066bccf9c69e492c90ee193388b8b7a
ms.sourcegitcommit: a1c086aa83d381129e62f9d8942f0fc889ffcab0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 08/05/2021
ms.locfileid: "115189604"
---
# <a name="eye-supported-target-selection"></a>Глаз — поддерживаемый Выбор целевого объекта

![MRTK](../../images/eye-tracking/mrtk_et_targetselect.png)

На этой странице обсуждаются различные варианты доступа к данным взгляда на глаза и события взгляда на глаза для выбора целевых объектов в МРТК. Отслеживание взгляда позволяет быстро и легко выбирать целевые объекты, используя сочетание сведений о том, что видят пользователи, с дополнительными входными данными, такими как _Отслеживание_ и _команды голоса_:

- Искать & скажите _"Select"_ (голосовая команда по умолчанию)
- Взгляните & скажите _"раскрыть"_ или _"Pop"_ (пользовательские голосовые команды)
- кнопка "найти & Bluetooth"
- Взгляните & сжатие (т. е. Держите руку перед вами и перенесите палец и проведите указатель пальца).
  - Обратите внимание, что чтобы это работало, [необходимо отключить лучи в руки](eye-tracking-eyes-and-hands.md#how-to-disable-the-hand-ray) .

Чтобы выбрать holographic содержимое с помощью глаза, существует несколько вариантов:

[**1. Используйте основной указатель фокуса:**](#1-use-generic-focus-and-pointer-handlers)

Он может быть понятен по приоритетному курсору.
По умолчанию, если руки находятся в представлении, это может быть некоторая рука.
Если в представлении нет ни одной руки, то указатель с приоритетом будет иметь вид "руководитель" или "глаз".
Таким образом, обратите внимание, что на основе текущей схемы проектирования или взгляда на глаза в качестве входных данных курсора поподавляется, если используются лучи.

Пример:

Пользователь хочет выбрать удаленную кнопку Holographic.
Разработчику необходимо предоставить гибкое решение, позволяющее пользователю выполнять эти задачи в различных условиях:

- Пошаговое руководство по кнопке
- Взгляните на него с расстояния и скажите «SELECT»
- Нацеливание на кнопку с помощью руки и выполнения сжатия в этом случае самым гибким решением является использование основного обработчика фокуса, так как он уведомляет вас каждый раз, когда основной указатель фокуса в данный момент инициирует событие. Обратите внимание, что если вы намерены использовать лучи, указатель фокуса будет отключен, как только руки появятся в представлении.

> [!IMPORTANT]
> Обратите внимание, что если вы намерены использовать лучи, указатель фокуса будет отключен, как только руки появятся в представлении. Если требуется поддержка [взаимодействия _"внешний вид и сжатие_ ", необходимо отключить руку](eye-tracking-eyes-and-hands.md#how-to-disable-the-hand-ray). В нашем примере с отслеживанием взглядов мы отключили руку, чтобы показать более широкие возможности взаимодействия с помощью глаз и движений руки — см. пример [позиционирования с поддержкой глаз](eye-tracking-eyes-and-hands.md).

[**2. одновременно используйте фокус глаза и лучи.**](#2-independent-eye-gaze-specific-eyetrackingtarget)

Могут существовать экземпляры, которые должны быть более конкретными, указывающие, какой тип указателей фокуса может активировать определенные события и разрешить одновременно использовать несколько методов взаимодействия.

Например, в приложении пользователь может использовать крайнее расстояние для управления некоторыми более отдаленными механическими настройками, например, захватить и удержать несколько удаленных частей holographic и удерживать их на месте. При этом пользователь должен пройти несколько инструкций и записать ход выполнения, пометив некоторые флажки. Если пользователь или его руки _не заняты_, это было бы инстинктуал, чтобы просто коснуться флажка или выбрать его с помощью руки. Тем не менее, если у пользователя есть и его руки, как в нашем случае с разделом с более тесными компонентами, необходимо предоставить пользователю возможность беспрепятственно прокручивать инструкции с помощью взгляда на глаза и просто взглянуть на флажок и сказать «проверить!».

Для этого необходимо использовать сценарий Эйетраккингтаржет, зависящий от глаза, который не зависит от основной МРТК Фокушандлерс и будет рассмотрен далее.

## <a name="1-use-generic-focus-and-pointer-handlers"></a>1. Использование универсальных обработчиков фокуса и указателей

Если отслеживание взгляда настроено правильно (см. раздел [Базовая настройка мртк для использования отслеживания взгляда](eye-tracking-basic-setup.md)), предоставление пользователям возможности выбирать самые голограммы в глаза будет таким же, как и для любого другого ввода фокуса (например, с помощью головного взгляда или руки). Это предоставляет отличное преимущество гибкого способа взаимодействия с голограммами путем определения основного типа фокуса в профиле указателя ввода МРТК в зависимости от потребностей пользователя, при этом код не затрагивается. Это позволяет переключаться между обработкой головного взгляда или глаза без изменения строки кода или замены руки с целью взгляда для взаимодействия.

### <a name="focusing-on-a-hologram"></a>Сосредоточение на голограмме

Чтобы определить, когда находится голограмма, используйте интерфейс _"имикседреалитифокушандлер"_ , который предоставляет два члена интерфейса: _онфокусентер_ и _онфокусексит_.

Ниже приведен простой пример из [колортап. CS](xref:Microsoft.MixedReality.Toolkit.Examples.Demos.EyeTracking.ColorTap) для изменения цвета голограммы при просмотре.

```c#
public class ColorTap : MonoBehaviour, IMixedRealityFocusHandler
{
    void IMixedRealityFocusHandler.OnFocusEnter(FocusEventData eventData)
    {
        material.color = color_OnHover;
    }

    void IMixedRealityFocusHandler.OnFocusExit(FocusEventData eventData)
    {
        material.color = color_IdleState;
    }
    ...
}
```

### <a name="selecting-a-focused-hologram"></a>Выбор голограммы с упором

Чтобы выбрать голограмму с фокусом, используйте _поинтерхандлер_ , чтобы прослушивать входные события для подтверждения выбора.
Например, если добавить _имикседреалитипоинтерхандлер_ , они будут реагировать на простые входные указатели.
Интерфейс _имикседреалитипоинтерхандлер_ требует реализации следующих трех членов интерфейса: _онпоинтеруп_, _онпоинтердовн_ и _онпоинтеркликкед_.

В приведенном ниже примере мы изменим цвет голограммы, взглянув на него и отменив или выполнив слово SELECT.
Действие, необходимое для запуска события, определяется тем, что `eventData.MixedRealityInputAction == selectAction` мы можем задать тип `selectAction` в редакторе Unity — по умолчанию это действие "Select". Типы доступных [микседреалитинпутактионс](../input-actions.md) можно настроить в профиле мртк с помощью действий ввода   ->  _входных_  ->  _данных_ профиля конфигурации мртк.

```c#
public class ColorTap : MonoBehaviour, IMixedRealityFocusHandler, IMixedRealityPointerHandler
{
    // Allow for editing the type of select action in the Unity Editor.
    [SerializeField]
    private MixedRealityInputAction selectAction = MixedRealityInputAction.None;
    ...

    void IMixedRealityPointerHandler.OnPointerUp(MixedRealityPointerEventData eventData)
    {
        if (eventData.MixedRealityInputAction == selectAction)
        {
            material.color = color_OnHover;
        }
    }

    void IMixedRealityPointerHandler.OnPointerDown(MixedRealityPointerEventData eventData)
    {
        if (eventData.MixedRealityInputAction == selectAction)
        {
            material.color = color_OnSelect;
        }
    }

    void IMixedRealityPointerHandler.OnPointerClicked(MixedRealityPointerEventData eventData) { }
}
```

### <a name="eye-gaze-specific-baseeyefocushandler"></a>Взгляд на Басиефокушандлер

Учитывая, что взгляд на глаза может сильно отличаться от других входных указателей, может возникнуть необходимость реагирования на ввод фокуса, если _взгляд_ воспринимается, и в данный момент является первичным указателем ввода.
Для этой цели следует использовать объект, [`BaseEyeFocusHandler`](xref:Microsoft.MixedReality.Toolkit.Input.BaseEyeFocusHandler) который относится к отслеживанию глаз и является производным от [`BaseFocusHandler`](xref:Microsoft.MixedReality.Toolkit.Input.BaseFocusHandler) .
Как упоминалось ранее, он срабатывает только в том случае, если нацеливание на глаз в настоящее время является первичным указателем (т. е. ни один входной луч не активен). Дополнительные сведения см. [в разделе Поддержка жестов глазного взгляда и руки](eye-tracking-eyes-and-hands.md).

Ниже приведен пример из `EyeTrackingDemo-03-Navigation` (Assets/мртк/examples/демонстрации/эйетраккинг/сцены).
В этой демонстрации есть две трехмерные голограммы, которые будут переноситься в зависимости от того, какая часть объекта будет выглядеть: Если пользователь просматривает левую часть голограммы, то эта часть будет медленно перемещаться к лицевой стороне пользователя.
Если просматривается правая сторона, то эта часть будет медленно перемещена на передний план.
Это поведение, которое может быть нежелательно, если вы не хотите, чтобы они были активны постоянно, а также что-то, что вы не хотите случайно активировать с помощью луча или головного взгляда.
[`OnLookAtRotateByEyeGaze`](xref:Microsoft.MixedReality.Toolkit.Examples.Demos.EyeTracking.OnLookAtRotateByEyeGaze)Если присоединиться, GameObject будет вращаться во время поиска.

```c#
public class OnLookAtRotateByEyeGaze : BaseEyeFocusHandler
{
    ...

    protected override void OnEyeFocusStay()
    {
        // Update target rotation
        RotateHitTarget();
    }

    ...

    ///
    /// This function computes the rotation of the target to move the currently
    /// looked at aspect slowly to the front.
    ///
    private void RotateHitTarget()
    {
        // Example for querying the hit position of the eye gaze ray using EyeGazeProvider
        Vector3 TargetToHit = (this.gameObject.transform.position - InputSystem.EyeGazeProvider.HitPosition).normalized;

        ...
    }
}
```

Полный список доступных событий можно найти в документации по API [`BaseEyeFocusHandler`](xref:Microsoft.MixedReality.Toolkit.Input.BaseEyeFocusHandler) :

- **Онэйефокусстарт:** Активируется после того, как элемент лучей глаза *начинает* пересекаться с этим объектом.
- **Онэйефокусстай:** Активируется, *когда* наблюдатель глаза пересекается с этим объектом.
- **Онэйефокусстоп:** Активируется после того, как наблюдатель глаза *перестанет* пересекаться с этим объектом.
- **Онэйефокусдвелл:** Активируется после того, как наблюдатель глаза пересекается с этим объектом в течение указанного промежутка времени.

## <a name="2-independent-eye-gaze-specific-eyetrackingtarget"></a>2. Независимый взгляд на Эйетраккингтаржет

Наконец, мы предоставляем решение, которое позволяет обрабатывать входные данные на основе взгляда полностью независимо от других указателей фокуса с помощью [`EyeTrackingTarget`](xref:Microsoft.MixedReality.Toolkit.Input.EyeTrackingTarget) сценария.

Это имеет три _преимущества_:

- Можно убедиться, что голограмма будет перереагировать только на взгляд пользователя.
- Это не зависит от текущего активного ввода. Таким образом, можно обрабатывать сразу несколько входов, например сочетание быстрого взгляда с жестами руки.
- Несколько событий Unity уже настроены для быстрой и удобной обработки и повторного использования существующих поведений в редакторе Unity или коде.

Существуют также некоторые _недостатки._

- Дополнительные усилия по обработке отдельных входов по отдельности.
- Без элегантного ухудшения: он поддерживает только определение целей для глаз. Если отслеживание взгляда не работает, потребуется дополнительное резервное действие.

Как и _басефокушандлер_, _эйетраккингтаржет_ поставляется с несколькими событиями Unity, специфичными для глаз, которые можно прослушать либо с помощью редактора Unity (см. пример ниже), либо с помощью _аддлистенер ()_ в коде:

- Онлукатстарт ()
- Вхилелукингаттаржет ()
- Онлукавай ()
- Ондвелл ()
- Selectd ()

В следующем примере мы рассмотрим несколько примеров использования _эйетраккингтаржет_.

### <a name="example-1-eye-supported-smart-notifications"></a>Пример #1: Поддерживаемые смарт-уведомления

В `EyeTrackingDemo-02-TargetSelection` (Assets/мртк/examples/эйетраккинг/сцены) можно найти пример для _"Smart внимательный Notifications"_ , который реагирует на взгляд.
Это трехмерные текстовые поля, которые можно поместить в сцену, что будет плавно увеличиваться и поворачиваться для удобства пользователя при просмотре. При чтении уведомления пользователь получает четкие и четко отображаемые данные. После того как вы прочитаете его и выйдете из уведомления, уведомление автоматически закрывается и исчезает. Для этого существует несколько сценариев универсального поведения, которые не относятся к отслеживанию глаз, например:

- [`FaceUser`](xref:Microsoft.MixedReality.Toolkit.Examples.Demos.EyeTracking.FaceUser)
- [`ChangeSize`](xref:Microsoft.MixedReality.Toolkit.Examples.Demos.EyeTracking.ChangeSize)
- [`BlendOut`](xref:Microsoft.MixedReality.Toolkit.Examples.Demos.EyeTracking.BlendOut)

Преимуществом этого подхода является то, что одни и те же скрипты могут повторно использоваться различными событиями. Например, голограмма может начинаться с пользователя на основе голосовых команд или после нажатия виртуальной кнопки. Чтобы активировать эти события, можно просто ссылаться на методы, которые должны выполняться в [`EyeTrackingTarget`](xref:Microsoft.MixedReality.Toolkit.Input.EyeTrackingTarget) скрипте, присоединенном к GameObject.

В качестве примера _"Smart внимательный Notifications"_ происходит следующее:

- **Онлукатстарт ()**: начинается уведомление...
  - *Фацеусер. привлекать:* ... Превратите пользователя в сторону.
  - *Чанжесизе. привлекать:* ... увеличение размера _(до указанного максимального масштаба)_.
  - *Наложение. привлекать:* ... начинает переходить в другое _состояние (после того, как оно находится в более тонком состоянии простоя)_.  

- **Ондвелл ()**: информирует сценарий _перехода_ о том, что уведомление было достаточно.

- **Онлукавай ()**: начинается уведомление...
  - *Фацеусер. ClassInterfaceAttribute отключите:* ... Вернитесь к исходной ориентации.
  - *Чанжесизе. ClassInterfaceAttribute отключите:* ... уменьшение до исходного размера.
  - *Наложение. ClassInterfaceAttribute отключите:* ... Начало смешения. Если _ондвелл ()_ был активирован, то разложение полностью и уничтожается, в противном случае обратно в состояние простоя.

**Вопросы проектирования:** Ключом к приятной работе здесь является тщательная настройка скорости любого из этих поведений, чтобы избежать возникновения дискомфорти, так как в этом случае не придется перереагировать на взгляд пользователя.
В противном случае это может быстро привести к чрезмерному переполнению.

<img src="../../images/eye-tracking/mrtk_et_EyeTrackingTarget_Notification.jpg" width="750" alt="Target Notification">

### <a name="example-2-holographic-gem-rotates-slowly-when-looking-at-it"></a>Пример #2: holographic драгоценный камень медленно вращается при просмотре

Как и в примере #1, мы можем легко создать отзыв о наведении для наших драгоценных камней в `EyeTrackingDemo-02-TargetSelection` (Assets/мртк/examples/эйетраккинг/сцены), который постепенно поворачивается в постоянном направлении и на постоянной скорости (в отличие от примера вращения выше) при просмотре. Все, что нужно, — активировать поворот holographic-драгоценного камень из события _вхилелукингаттаржет ()_ _эйетраккингтаржет_. Ниже приведены некоторые дополнительные сведения.

1. Создайте универсальный скрипт, содержащий открытую функцию для поворота GameObject, к которой он присоединен. Ниже приведен пример из _ротатевисконстспиддир. CS_ , где можно настроить направление вращения и скорость работы в редакторе Unity.

    ```c#
    using UnityEngine;

    namespace Microsoft.MixedReality.Toolkit.Examples.Demos.EyeTracking
    {
        /// <summary>
        /// The associated GameObject will rotate when RotateTarget() is called based on a given direction and speed.
        /// </summary>
        public class RotateWithConstSpeedDir : MonoBehaviour
        {
            [Tooltip("Euler angles by which the object should be rotated by.")]
            [SerializeField]
            private Vector3 RotateByEulerAngles = Vector3.zero;

            [Tooltip("Rotation speed factor.")]
            [SerializeField]
            private float speed = 1f;

            /// <summary>
            /// Rotate game object based on specified rotation speed and Euler angles.
            /// </summary>
            public void RotateTarget()
            {
                transform.eulerAngles = transform.eulerAngles + RotateByEulerAngles * speed;
            }
        }
    }
    ```

1. Добавьте [`EyeTrackingTarget`](xref:Microsoft.MixedReality.Toolkit.Input.EyeTrackingTarget) скрипт в целевую GameObject и сослаться на функцию _ротатетаржет ()_ в триггере унитевент, как показано на следующем снимке экрана:

    ![Пример Эйетраккингтаржет](../../images/eye-tracking/mrtk_et_EyeTrackingTargetSample.jpg)

### <a name="example-3-pop-those-gems-aka-_multi-modal-eye-gaze-supported-target-selection_"></a>Пример #3: POP эти драгоценные камни представляют собой _поддерживаемый целевой объект с несколькими модальными глазами-взглядами_

В предыдущем примере мы показали, насколько просто определить, просматривается ли цель и как активировать реакцию на нее. Далее создадим драгоценные камни, развернутые с помощью события _OnSelected ()_ из [`EyeTrackingTarget`](xref:Microsoft.MixedReality.Toolkit.Input.EyeTrackingTarget) . Интересная часть заключается в *том, как* активируется выбор. [`EyeTrackingTarget`](xref:Microsoft.MixedReality.Toolkit.Input.EyeTrackingTarget)Позволяет быстро назначать различные способы вызова выбора:

- _Жест сжатия_: Если задать для параметра "выбрать действие" значение "Select", будет использоваться жест по умолчанию, чтобы активировать выбор.
Это означает, что пользователь может просто поднять свою руку и просжатие бегунка и указатель пальца, чтобы подтвердить выбор.

- Скажите _"выбрать"_: используйте голосовую команду по умолчанию _"Select"_ для выбора голограммы.

- Скажем, _"взрыв_ " или _"Pop"_: для использования пользовательских голосовых команд необходимо выполнить два шага:
    1. Настройка настраиваемого действия, например _"дестройтаржет"_
        - Перейдите в раздел _мртк-> input-> input Actions_ .
        - Щелкните "добавить новое действие".

    2. Настройте команды Voice, которые активируют это действие, например _"раскрыть"_ или _"Pop"_ .
        - Перейдите к _мртк-> input-> Speech_
        - Щелкните "добавить новую голосовую команду".
            - Связывание только что созданного действия
            - Назначение _keyCode_ для включения действия с помощью нажатия кнопки

![Образец Эйетраккингтаржет Voice Commands](../../images/eye-tracking/mrtk_et_voicecmdsample.jpg)

Если выбран драгоценный камень, он разворачивается, выполнит звук и исчезнет. Это обрабатывается [`HitBehaviorDestroyOnSelect`](xref:Microsoft.MixedReality.Toolkit.Examples.Demos.EyeTracking.HitBehaviorDestroyOnSelect) сценарием. Имеются две возможности.

- **В редакторе Unity:** Можно просто связать сценарий, прикрепленный к каждому из наших шаблонов, с событием Unity () в редакторе Unity.
- **В коде:** Если вы не хотите перетаскивать объекты gameobject, вы также можете просто добавить прослушиватель событий непосредственно в сценарий.  
Ниже приведен пример того, как мы сделали это в [`HitBehaviorDestroyOnSelect`](xref:Microsoft.MixedReality.Toolkit.Examples.Demos.EyeTracking.HitBehaviorDestroyOnSelect) сценарии:

```c#
/// <summary>
/// Destroys the game object when selected and optionally plays a sound or animation when destroyed.
/// </summary>
[RequireComponent(typeof(EyeTrackingTarget))] // This helps to ensure that the EyeTrackingTarget is attached
public class HitBehaviorDestroyOnSelect : MonoBehaviour
{
    ...
    private EyeTrackingTarget myEyeTrackingTarget = null;

    private void Start()
    {
        myEyeTrackingTarget = this.GetComponent<EyeTrackingTarget>();

        if (myEyeTrackingTarget != null)
        {
            myEyeTrackingTarget.OnSelected.AddListener(TargetSelected);
        }
    }

    ...

    ///
    /// This is called once the EyeTrackingTarget detected a selection.
    ///
    public void TargetSelected()
    {
        // Play some animation
        // Play some audio effect
        // Handle destroying the target appropriately
    }
}
```

### <a name="example-4-use-hand-rays-and-eye-gaze-input-together"></a>Пример #4: входные данные с использованием луча и глазного взгляда

Лучи имеют приоритет над нацеливанием на головной взгляд и глаз. Это означает, что если вы наберете лучи, то в то время, когда они появятся, руку будет действовать как основной указатель.
Однако могут возникнуть ситуации, в которых вы хотите использовать лучи и при этом определить, просматриваете ли пользователь определенную голограмму. Легко! По сути, требуется выполнить два действия:

**1. включение руки** . чтобы включить луч, перейдите в раздел _смешанная реальность набор средств-> входные указатели >_.
в _эйетраккингдемо-00-рутсцене_ , где набор средств смешанной реальности настраивается один раз для всех демонстрационных сцен отслеживания взгляда, вы должны увидеть _эйетраккингдемопоинтерпрофиле_.
Можно либо создать новый _входной профиль_ с нуля, либо адаптировать отслеживание текущего взгляда:

- **С нуля:** На вкладке _указатели_ выберите _дефаултмикседреалитинпутпоинтерпрофиле_ в контекстном меню.
Это профиль указателя по умолчанию, в котором уже включен параметр руки.
Чтобы изменить курсор по умолчанию (непрозрачную белую точку), достаточно клонировать профиль и создать собственный профиль пользовательского указателя.
Затем замените _дефаулткурсор_ на _эйегазекурсор_ в разделе _Взгляните prefab_.  
- **На основе существующих _эйетраккингдемопоинтерпрофиле_:** дважды щелкните _эйетраккингдемопоинтерпрофиле_ и добавьте следующую запись в разделе _Параметры указателя_:
  - **Тип контроллера:** "Руки", "Windows Mixed Reality"
  - **Правой или левой:** Всеми
  - **Prefab указателя:** дефаултконтроллерпоинтер

**2. Обнаружение голограммы:** используйте [`EyeTrackingTarget`](xref:Microsoft.MixedReality.Toolkit.Input.EyeTrackingTarget) сценарий, чтобы разрешить обнаружение голограммы, как описано выше. Вы также можете взглянуть на `FollowEyeGaze` Пример сценария для создания идей, так как это показывает голограмму, следующую за взглядом на глаза (например, курсором), включены ли лучи.

Теперь, когда вы запускаете демонстрационные сцены отслеживания взгляда, вы должны увидеть луч, поступающий из руки.
Например, в демонстрационном примере выбора цели отслеживания взгляда полупрозрачный круг по-прежнему отслеживается на взгляде глаза, а драгоценные камни реагируют на то, где они просматривается, а в меню верхней сцены используется первичный указатель ввода (руки).

---
[Вернуться к "Отслеживание взглядов в Микседреалититулкит"](eye-tracking-main.md)
